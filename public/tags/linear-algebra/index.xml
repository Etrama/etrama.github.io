<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linear Algebra on Old Habits AI Hard</title>
    <link>https://etrama.github.io/tags/linear-algebra/</link>
    <description>Recent content in Linear Algebra on Old Habits AI Hard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 07 Oct 2023 18:36:11 -0400</lastBuildDate><atom:link href="https://etrama.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Demystifying the mathematics behind PCA</title>
      <link>https://etrama.github.io/posts/2023-10-07-pca/</link>
      <pubDate>Sat, 07 Oct 2023 18:36:11 -0400</pubDate>
      
      <guid>https://etrama.github.io/posts/2023-10-07-pca/</guid>
      <description>View this post in a Jupyter Notebook
Demystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.
Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features.</description>
    </item>
    
  </channel>
</rss>
