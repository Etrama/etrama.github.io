[{"content":"View this post in a Jupyter Notebook\nDemystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.\nUntil I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features. The interviewer was even kind enough to throw me a hint about projections. To my chagrin, I couldn\u0026rsquo;t figure it out even then and had to admit I was stumped. I knew that I was taught the mathematics behind PCA during the first year of my Master\u0026rsquo;s course, which made it feel worse. So here\u0026rsquo;s a post to make sure that doesn\u0026rsquo;t happen to you, dear reader, and hopefully me as well üòÜ.\nWARNING: This post will be long and mathematical as the title suggests.\nIntroducing PCA PCA stands for Principal Component Analysis. If you have multidimensional data that\u0026rsquo;s giving you a hard time when you try to train a model on it, PCA could be the answer. You could also visualize high dimensional data using PCA, which is done often in NLP.\n# there\u0026#39;s no understanding without doing, so let\u0026#39;s write some code import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as mcolors import seaborn as sns import random sns.set_style(\u0026#34;darkgrid\u0026#34;) from sklearn.datasets import load_wine np.random.seed(420) We have a set of data, that we will call $x$. $x$ is $(k,m)$ dimensional, hence every point in $x$ will be such that $x_i \\in \\mathbb{R}^m$ (just a fancy way of saying we pick a random point from x and that point is a real number in $m$ dimensions). We have $k$ such $m$ dimensional points. Our goal is to represent $x$ in less than $m$ dimensions. This is by no means going to be a perfect transition. Say we project down from $m$ dimensions to $n$ dimensions. By definition, $n\u0026lt;m$.\n# say D=10, i.e we have 10 dimensional data. x = np.random.rand(10,10) # for the geeks amongst us this is from a continuous uniform distrbution from [0, 1) x.shape # so we have 10 (k=10) data points each of which have 10 dimensions (m=10) (10, 10)\rThe typical approach is to maximize the variance of the $n$ dimensional data in such a way that it captures as much of the variance of the $m$ dimensional data as possible. Let\u0026rsquo;s call $z$ the co-ordinates of the projection of $x$ into the lower $n$ dimensional space.\nHarold Hotelling is credited with coming up with the idea of minimizing the variance in 1933. Mr.Hotelling was quite the madlad, since he also came up with the T-square distribution amongst other laws, lemmas and rules. He wrote presciently in his original paper, 90 years ago:\n\u0026ldquo;It is natural to ask whether some more fundamental set of independent variables exists, perhaps fewer in number than the $x$\u0026rsquo;s, which determine the values the $x$\u0026rsquo;s will take.\u0026rdquo;\nGentle math Let\u0026rsquo;s introduce the mean. This is our \u0026ldquo;average\u0026rdquo; friend. $$\\bar{x}=\\frac{1}{n}\\sum_{i}^{n}x_i$$\n# in numpy if we don\u0026#39;t define the axis # it will compute the mean of all the 100 elements that we entered x.mean() 0.48955083684170964\r# since we\u0026#39;re dealing with components, # we are more interested in the columwise or featurewise mean x.mean(axis=0) array([0.50299989, 0.46627609, 0.62990129, 0.5188987 , 0.43764572,\r0.59755813, 0.48329069, 0.47277132, 0.46266469, 0.32350185])\r# if you\u0026#39;re paranoid and you\u0026#39;d like to check the values, here you go: for i in range(0,10): print(x[:,i].sum()/10) 0.5029998856345316\r0.4662760922130609\r0.6299012923571106\r0.5188986993892943\r0.4376457212611754\r0.5975581266463694\r0.48329069068570707\r0.47277132022054413\r0.46266468509761466\r0.3235018549116885\rStandard deviation The standard deviation is just how much each point deviates from the mean. It measures how spread out our data is. Standard deviation is denoted as $\\sigma$.\n$$\\sigma=\\sqrt{\\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})^2}$$\nx.std(axis=0) array([0.25282779, 0.31208093, 0.24974188, 0.31661173, 0.30802279,\r0.25662288, 0.1512335 , 0.22761326, 0.22715909, 0.3097734 ])\r# say we have some different data: y = np.array([1]*6) z = np.array([0]*3+[1]*3) w = np.arange(1,7) arrays = [y, z, w] # we can check their means: for _ in arrays: print(_.mean()) 1.0\r0.5\r3.5\r# we can also check their standard deviation: for _ in arrays: print(_.std()) 0.0\r0.5\r1.707825127659933\r# paranoia tax for _ in arrays: current_mean = _.mean() print(np.sqrt(np.sum((_ - current_mean)**2)/len(_))) 0.0\r0.5\r1.707825127659933\rThis makes sense when you consider that our array y had only one value and has a standard deviation of 0. Our array z was slightly more spread out, but still it only had 2 unique values each occuring thrice. Our array w was the most numerically diverse of y, z and w, and hence it had the highest standard deviation.\nVariance The variance is simply the square of the standard deviation.\n$$ var(x) = \\sigma^2=\\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})^2$$\nx.var(axis=0) array([0.06392189, 0.09739451, 0.06237101, 0.10024299, 0.09487804,\r0.06585531, 0.02287157, 0.0518078 , 0.05160125, 0.09595956])\r# consider the 1d line data again for _ in arrays: print(_.var()) 0.0\r0.25\r2.9166666666666665\rAll the measures we looked at so far were dealing with only one dimension. Notice that we have just one number to describe the mean, $\\sigma$ and $\\sigma^2$ for our one-liner data (y,z,w), since they contain only one dimension. However, we had to specify an axis for $x$, since $x$ is a collection of 10 data points with 10 dimensions each. Also notice that when dealing with $x$, all 3 measures had 10 values, that is as many values as the number of dimensions in $x$.\nCovariance To figure out the interactions between the dimensions and how they vary depending on each other, we introduce the aptly named covariance term. The covariance is always measured between two dimensions. Notice the similarities between the variance and the covariance below:\n$$ var(x) \\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})(x_i - \\bar{x})$$\n$$ cov(x,y) = \\frac{1}{n}\\sum_{i}^{n}(x_i - \\bar{x})(y_i - \\bar{y})$$\nIt might be nice to see the covariance of variable on a real dataset rather than the random data we have here:\n# toy data from the wine sklearn dataset from sklearn.datasets import load_wine data = load_wine()[\u0026#34;data\u0026#34;] data.shape # we have 13 features (178, 13)\rCovariance can only capture the variance between 2 variables. Additionally, we also know that the covarince of a feature with itself is the variance. As far as a count of covariances goes, in this case with 13 features, we will have a 13x13 matrix, since there 12 other features to compare with and one of the feature compared with itself (variance). All of the above mental gymnastics just allows us to express covariance a matrix. The covariance matrix is usually denoted by $\\Sigma$.\n# features are in columns, hence rowvar is false np.cov(data, rowvar=False).shape (13, 13)\rnp.cov(data, rowvar=False) array([[ 6.59062328e-01, 8.56113090e-02, 4.71151590e-02,\r-8.41092903e-01, 3.13987812e+00, 1.46887218e-01,\r1.92033222e-01, -1.57542595e-02, 6.35175205e-02,\r1.02828254e+00, -1.33134432e-02, 4.16978226e-02,\r1.64567185e+02],\r[ 8.56113090e-02, 1.24801540e+00, 5.02770393e-02,\r1.07633171e+00, -8.70779534e-01, -2.34337723e-01,\r-4.58630366e-01, 4.07333619e-02, -1.41146982e-01,\r6.44838183e-01, -1.43325638e-01, -2.92447483e-01,\r-6.75488666e+01],\r[ 4.71151590e-02, 5.02770393e-02, 7.52646353e-02,\r4.06208278e-01, 1.12293658e+00, 2.21455913e-02,\r3.15347299e-02, 6.35847140e-03, 1.51557799e-03,\r1.64654327e-01, -4.68215451e-03, 7.61835841e-04,\r1.93197391e+01],\r[-8.41092903e-01, 1.07633171e+00, 4.06208278e-01,\r1.11526862e+01, -3.97476036e+00, -6.71149146e-01,\r-1.17208281e+00, 1.50421856e-01, -3.77176220e-01,\r1.45024186e-01, -2.09118054e-01, -6.56234368e-01,\r-4.63355345e+02],\r[ 3.13987812e+00, -8.70779534e-01, 1.12293658e+00,\r-3.97476036e+00, 2.03989335e+02, 1.91646988e+00,\r2.79308703e+00, -4.55563385e-01, 1.93283248e+00,\r6.62052061e+00, 1.80851266e-01, 6.69308068e-01,\r1.76915870e+03],\r[ 1.46887218e-01, -2.34337723e-01, 2.21455913e-02,\r-6.71149146e-01, 1.91646988e+00, 3.91689535e-01,\r5.40470422e-01, -3.50451247e-02, 2.19373345e-01,\r-7.99975192e-02, 6.20388758e-02, 3.11021278e-01,\r9.81710573e+01],\r[ 1.92033222e-01, -4.58630366e-01, 3.15347299e-02,\r-1.17208281e+00, 2.79308703e+00, 5.40470422e-01,\r9.97718673e-01, -6.68669999e-02, 3.73147553e-01,\r-3.99168626e-01, 1.24081969e-01, 5.58262255e-01,\r1.55447492e+02],\r[-1.57542595e-02, 4.07333619e-02, 6.35847140e-03,\r1.50421856e-01, -4.55563385e-01, -3.50451247e-02,\r-6.68669999e-02, 1.54886339e-02, -2.60598680e-02,\r4.01205097e-02, -7.47117692e-03, -4.44692440e-02,\r-1.22035863e+01],\r[ 6.35175205e-02, -1.41146982e-01, 1.51557799e-03,\r-3.77176220e-01, 1.93283248e+00, 2.19373345e-01,\r3.73147553e-01, -2.60598680e-02, 3.27594668e-01,\r-3.35039177e-02, 3.86645655e-02, 2.10932940e-01,\r5.95543338e+01],\r[ 1.02828254e+00, 6.44838183e-01, 1.64654327e-01,\r1.45024186e-01, 6.62052061e+00, -7.99975192e-02,\r-3.99168626e-01, 4.01205097e-02, -3.35039177e-02,\r5.37444938e+00, -2.76505801e-01, -7.05812576e-01,\r2.30767480e+02],\r[-1.33134432e-02, -1.43325638e-01, -4.68215451e-03,\r-2.09118054e-01, 1.80851266e-01, 6.20388758e-02,\r1.24081969e-01, -7.47117692e-03, 3.86645655e-02,\r-2.76505801e-01, 5.22449607e-02, 9.17662439e-02,\r1.70002234e+01],\r[ 4.16978226e-02, -2.92447483e-01, 7.61835841e-04,\r-6.56234368e-01, 6.69308068e-01, 3.11021278e-01,\r5.58262255e-01, -4.44692440e-02, 2.10932940e-01,\r-7.05812576e-01, 9.17662439e-02, 5.04086409e-01,\r6.99275256e+01],\r[ 1.64567185e+02, -6.75488666e+01, 1.93197391e+01,\r-4.63355345e+02, 1.76915870e+03, 9.81710573e+01,\r1.55447492e+02, -1.22035863e+01, 5.95543338e+01,\r2.30767480e+02, 1.70002234e+01, 6.99275256e+01,\r9.91667174e+04]])\rPositive covariance values between two features indicate that as one feature increases the other also increases. Negative values indicate an increase-decrease relation.\nThe 13x13 matrix might be a little hard to visualize, so let\u0026rsquo;s look at a covariance matrix with just 4 features, $a, b, c, d$: $$ \\Sigma = \\begin{bmatrix} cov(a,a) \u0026amp; cov(a,b) \u0026amp; cov(a,c) \u0026amp; cov(a,d) \\\\ cov(b,a) \u0026amp; cov(b,b) \u0026amp; cov(b,c) \u0026amp; cov(b,d) \\\\ cov(c,a) \u0026amp; cov(c,b) \u0026amp; cov(c,c) \u0026amp; cov(c,d) \\\\ cov(d,a) \u0026amp; cov(d,b) \u0026amp; cov(d,c) \u0026amp; cov(d,d) \\ \\end{bmatrix} $$\nProof that the covariance matrix is symmetric $$cov(a,b) = cov(b,a)$$ This is quite easy to prove, consider: $$ cov(a,b) = \\frac{1}{n}\\sum_{i}^{n}(a_i - \\bar{a})(b_i - \\bar{b})$$ $$ cov(b,a) = \\frac{1}{n}\\sum_{i}^{n}(b_i - \\bar{b})(a_i - \\bar{a})$$ We know that multiplication is commutative, i.e. $ab = ba$. This makes the covariance matrix a symmetric matrix.üòÅ\nSerious math Let\u0026rsquo;s talk about matrices and their products. We can multiple two matrices $\\mathbf{A}$ with dimensions (x,y) and $\\mathbf{B}$, with dimensions (u,v), if and only if, y = u, i.e. the number of columns in $\\mathbf{A}$ is the same as the number of rows in $\\mathbf{B}$. The matrix product itself in this case will be (x,v) dimensional.\n# let\u0026#39;s say I have two matrices and I try to multiply them: a = np.random.rand(10,10) b = np.random.rand(10,20) a.dot(b).shape (10, 20)\rEvery linear transformation $T: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ can be represented as matrix. Going from $m$ to $n$ dimensions? I bet that sounds kinda familiar. PCA is nothing but a linear transformation.\nEigenvalues and Eigenvectors Say we have a vector $x$ and a matrix $\\mathbf{A}$. Almost all vectors change direction when multipled by a matrix, i.e. they point in a different direction. When this multiplication with a matrix does not change the direction of $x$, then $x$ is called an eigenvector. $$\\mathbf{A}x = \\lambda x$$ $\\lambda$ is the eigenvalue of the matrix $\\mathbf{A}$. While $\\mathbf{A}$ may not change the direction of x, it does change the magnitude. The eigenvalue $\\lambda$ tells us whether $\\mathbf{A}$ increased / decreased / did neither vis-a-vis the magnitude of the eigenvector $x$.\nEigenvalues visualized Right now, we kind of understand when we can multiply two matrices. The same principles apply when we\u0026rsquo;re multiplying a matrix with a vector.\n# for instance: matrix1 = np.random.rand(3,3) vector1 = np.random.rand(3,1) # we can multiply matrix1 and vector1 because: (3x3)x(3x1), # both the inner dimensions are the same just as in the # matrix to matrix multiplication matrix1.dot(vector1) array([[0.52496104],\r[1.01737068],\r[1.03492964]])\rI found I daresay my favorite example and demonstration of this concept in book called \u0026ldquo;The Manga Guide to Linear Algebra\u0026rdquo;. With thanks, I\u0026rsquo;ll be using the same example as the book.\nWe know that we can multiply vectors and matrices under a certain condition on their dimensions. But what happens when we do multiply a matrix and a vector?\n# Consider the following two vectors: vector1 = np.array([3, 1]) # represents the point, x=3, y=1 vector2 = np.array([1, 2]) # represents the point x=1, y=2 np.array([vector1, vector2]) array([[3, 1],\r[1, 2]])\rdef plot_vectors(list_of_vectors, plot_title): # https://saturncloud.io/blog/how-to-plot-vectors-in-python-using-matplotlib-a-guide-for-data-scientists/ fig, ax = plt.subplots() # colors = list(\u0026#34;bgrcmy\u0026#34;) for vector in list_of_vectors: ax.quiver(0, 0, vector[0], vector[1], angles=\u0026#39;xy\u0026#39;, scale_units=\u0026#39;xy\u0026#39;, scale=1, color=random.choice(list(mcolors.TABLEAU_COLORS.values()) )) xlim = max(np.array(list_of_vectors).max(axis=0)) ylim = max(np.array(list_of_vectors).max(axis=1)) ax.set_xlim([-xlim-1, xlim+1]) ax.set_ylim([-ylim-1, ylim+1]) # plt.grid() plt.xlabel(\u0026#34;x axis\u0026#34;) plt.ylabel(\u0026#34;y axis\u0026#34;) plt.title(f\u0026#34;Figure: {plot_title}\u0026#34;) plt.show() plot_vectors([vector1, vector2], \u0026#34;Initial Vectors\u0026#34;) # say we multiply these vectors by a 2x2 matrix matrix1 = np.array([[8, -3],[2, 1]]) matrix1 array([[ 8, -3],\r[ 2, 1]])\rtransformed_vector1 = matrix1.dot(vector1) transformed_vector2 = matrix1.dot(vector2) plot_vectors([transformed_vector1, transformed_vector2], \u0026#34;Transformed Vectors\u0026#34;) # we can see that the matrix has changed the initial vectors plot_vectors([vector1, vector2, transformed_vector1, transformed_vector2], \u0026#34;All vectors\u0026#34;) The rotation is more evident when we look at a plane. Borrowing an example from the Manga Guide to Linear Algebra book (follow the fish):\nInitially, the vectors look like:\nAfter the matrix multiplication, the vectors look like:\nThere\u0026rsquo;s something interesting to notice in the graphs and the pictures. Notice that the plane rotates when we transform it linearly (multiply a matrix). Interestingly, the two vectors that we chose didn\u0026rsquo;t rotate at all. This means all points on the original fish that were not on the lines shown in images were not just extended, but also rotated. However, the points on the two vectors that we chose were just extended. That\u0026rsquo;s because they\u0026rsquo;re special vectors with respect to this transformation (with respect to this matrix). These two vectors are the eigenvectors of the matrix, as multiplying them by the matrix given only changes their magnitude and not their direction. Which provides some intuition as to why we have: $$\\mathbf{A}x = \\lambda x$$\nWhen we have eigenvectors of the form shown in the graphs and pictures, we have eigenvalues associated with them. Eigenvectors and eigenvalues come in a pair. A matrix with dimensions $(m,m)$, can have at most $m$ eigenvectors and by extension $m$ eigenvalues.\nWe haven\u0026rsquo;t yet found the eigenvalues for the example above. Let\u0026rsquo;s try to generalize it.\nEigenvalues generalized Starting with the equation we know: $$\\mathbf{A}x = \\lambda x$$ Moving the terms to one side: $$\\mathbf{A} x-\\lambda x=0$$ Factoring out the $x$, $I$ is the identity matrix. Realize that $\\lambda$ is essentially a \u0026ldquo;scalar\u0026rdquo; since it just \u0026ldquo;scales\u0026rdquo; the vector $x$: $$(\\mathbf{A}-\\lambda \\mathbf{I}) x=0$$ For solutions where $x \\neq 0$, we have: $$\\operatorname{det}(A-\\lambda \\mathbf{I})=0$$ We can only define determinants for square matrices.\n$$ \\mathbf{A} = \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\dots \u0026amp; a_{2m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\dots \u0026amp; a_{mm} \\end{bmatrix} $$ $$ \\lambda \\mathbf{I} = \\begin{bmatrix} \\lambda \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; \\dots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; \\lambda \\end{bmatrix} $$\n$$ \\mathbf{A} - \\lambda \\mathbf{I} = \\begin{bmatrix} a_{11} - \\lambda \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\\\ a_{21} \u0026amp; a_{22} - \\lambda \u0026amp; \\dots \u0026amp; a_{2m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\dots \u0026amp; a_{mm} - \\lambda \\end{bmatrix} $$\n$$ \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = \\begin{vmatrix} a_{11} - \\lambda \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\\\ a_{21} \u0026amp; a_{22} - \\lambda \u0026amp; \\dots \u0026amp; a_{2m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\dots \u0026amp; a_{mm} - \\lambda \\end{vmatrix} $$\nWe know that $\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0$, hence: $$ \\begin{vmatrix} a_{11} - \\lambda \u0026amp; a_{12} \u0026amp; \\dots \u0026amp; a_{1m} \\\\ a_{21} \u0026amp; a_{22} - \\lambda \u0026amp; \\dots \u0026amp; a_{2m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\dots \u0026amp; a_{mm} - \\lambda \\end{vmatrix} = 0$$\nApplying this general formula to our example:\n$$\\mathbf{A} = \\begin{vmatrix} 8 \u0026amp; -3 \\\\ 2 \u0026amp; 1 \\ \\end{vmatrix}$$\n$$ \\begin{vmatrix} 8 - \\lambda \u0026amp; - 3 \\\\ 2 \u0026amp; 1 - \\lambda \\ \\end{vmatrix} = 0$$\n$$ (8 - \\lambda)(1 - \\lambda) - (-3)(2) = 0$$\n$$ 8 - 8\\lambda - \\lambda + \\lambda^2 + 6 = 0$$\n$$ \\lambda^2 - 9\\lambda + 14 = 0$$\nUsing the quadratic formula, we get: $$ \\lambda_1, \\lambda_2 = \\frac{9 \\pm \\sqrt{81 - 56}}{2}$$\n$$ \\lambda_1, \\lambda_2 = \\frac{9 \\pm 5}{2}$$\n$$ \\lambda_1 = 7, \\lambda_2 = 2$$\nWe have our eigenvalues. Neat. We already know the eigenvectors, since we chose them. However, if we didn\u0026rsquo;t, we could find them using the equation $(\\mathbf{A}-\\lambda \\mathbf{I})x=0$ or even $\\mathbf{A}x =\\lambda x$. Considering $\\lambda_1 = 7$:\n$$ \\begin{bmatrix} 8 - 7 \u0026amp; - 3 \\\\ 2 \u0026amp; 1 - 7 \\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$ \\begin{bmatrix} 1 \u0026amp; - 3 \\\\ 2 \u0026amp; -6 \\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$ \\begin{bmatrix} x_1 \u0026amp; - 3 x_2 \\\\ 2 x_1 \u0026amp; -6 x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\nWe observe that the second row is just a multiple of the first row. This means that the second row is redundant and we can ignore it.\n$$ \\begin{bmatrix} x_1 \u0026amp; - 3 x_2 \\\\ 0 \u0026amp; 0 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$x_1 = 3 x_2$$\nHence, $$x = \\begin{bmatrix} 3 x_2 \\\\ x_2 \\ \\end{bmatrix}$$\nFactorizing x_2, we get, $$x = x_2 \\begin{bmatrix} 3 \\\\ 1 \\ \\end{bmatrix}$$\nBy varying x_2, we will obtain the different scaled versions of the eigenvector. Which leaves with with the eigenvector: [3, 1] expressed in x-y cartesian co-ordinates as x = 3, y = 1. Which is the same as vector_1 üòÑ! Yay!\nWe do the same thing with $\\lambda_2=2$: $$ \\begin{bmatrix} 8 - 2 \u0026amp; - 3 \\\\ 2 \u0026amp; 1 - 2 \\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$ \\begin{bmatrix} 6 \u0026amp; - 3 \\\\ 2 \u0026amp; -1 \\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$ \\begin{bmatrix} 6 x_1 \u0026amp; - 3 x_2 \\\\ 2 x_1 \u0026amp; -1 x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\nThe top row is a multiple of the bottom row, ignoring: $$ \\begin{bmatrix} 0 \u0026amp; 0 \\\\ 2 x_1 \u0026amp; -1 x_2 \\ \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\ \\end{bmatrix}$$\n$$2 x_1 = x_2$$\n$$x = \\begin{bmatrix} x_1 \\\\ 2 x_1 \\ \\end{bmatrix}$$\nFactorizing x_1, we get, $$x = x_1 \\begin{bmatrix} 1 \\\\ 2 \\ \\end{bmatrix}$$\nAnd we have our second eigenvector: [1, 2] expressed in x-y cartesian co-ordinates as x = 1, y = 2. Which is the same as vector_2!\nPowers of a matrix Say we had to calculate $2^{12}$ and we weren\u0026rsquo;t allowed to use a calculator. We could multiply 2 by hand 12 times, or we could try to be smart and use the fact that $2^{12} = 2^4 * 2^4 * 2^4$. But what if I had to find the 12th power of a matrix? Even multiplying a matrix a couple of times by hand is a pain. We need a trick, much like the one we used for $2^{12}$.\nWhat if we had a m-dimensional square diagonal matrix (mxm) like the one below: $$\\mathbf{D} = \\begin{bmatrix} d_1 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\ 0 \u0026amp; d_2 \u0026amp; \\dots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; d_m \\end{bmatrix}$$\nIt would be quite easy to raise this to the second power: $$\\mathbf{D}^2 = \\begin{bmatrix} d_1^2 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\ 0 \u0026amp; d_2^2 \u0026amp; \\dots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; d_m^2 \\end{bmatrix}$$\nOr even the 100th power: $$\\mathbf{D}^{100} = \\begin{bmatrix} d_1^{100} \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\ 0 \u0026amp; d_2^{100} \u0026amp; \\dots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; d_m^{100} \\end{bmatrix}$$\nDiagonal matrices and eigenvalues Diagonal matrices are nice, since they only change the magnitude of a vector. For instance: $$\\begin{bmatrix} d_1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; d_2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_3 \\end{bmatrix} \\begin{bmatrix} a_1 \\\\ 0 \\\\ 0 \\ \\end{bmatrix} = d_1\\begin{bmatrix} a_1 \\\\ 0 \\\\ 0 \\ \\end{bmatrix} $$\n$$\\begin{bmatrix} d_1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; d_2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ b_1 \\\\ 0 \\ \\end{bmatrix} = d_2\\begin{bmatrix} 0 \\\\ b_1 \\\\ 0 \\ \\end{bmatrix} $$\n$$\\begin{bmatrix} d_1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; d_2 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; d_3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ c_1 \\ \\end{bmatrix} = d_3\\begin{bmatrix} 0 \\\\ 0 \\\\ c_1 \\ \\end{bmatrix} $$\nWhat if we had a diagonal matrix that was only composed of eigenvalues? That would be quite nice and it seems like concept comes about naturally since diagonal matrix also scale vectors, just like eigenvalue of a matrix scale the eigenvectors.\nFrom $\\mathbf{A}x = \\lambda x$, we know that for an eigenvector $x$, multiplication by a matrix is the same as being multiplied by a scalar $\\lambda$. Multiplying a matrix and its eigenvector several times would be the same as multiplying the scalar $\\lambda$ several times.\nWe still need to figure out how to find higher powers for non-diagonal matrices. Perhaps we could factorize this non-diagonal matrix into a product of other matrices including a diagonal matrix? Maybe we could use the eigenvectors too since their scaling laws are known ? Turns out there\u0026rsquo;s a process to do just that, called diagonalization. Starting out with the familiar eigenvalue equation: $$\\mathbf{A} \\mathbf{X}= \\lambda x \\tag{Eigenvalue equation}$$\nThe magic equation turns out to \u0026ldquo;look\u0026rdquo; very similar: $$\\mathbf{A} \\mathbf{X}= \\mathbf{X} \\mathbf{\\Lambda} \\tag{Diagonalization equation}$$\n$\\mathbf{A}$ is the original square matrix that we\u0026rsquo;ve been dealing with. But unlike the eigenvalue equation where $x$ was a vector and $\\lambda$ was a scalar, $\\mathbf{X}$ and $\\Lambda$ are both matrices. $\\mathbf{X}$ is matrix containing the eigenvectors of $\\mathbf{A}$ as its columns. This equation will help us factorize $\\mathbf{A}$ into a product of other matrices. Please note that in this case $\\mathbf{X}$ is not the input matrix, just a generic matrix representing the eigenvectors of $\\mathbf{A}$.\nMathematical magic, starting with the diagonalization equation: $$\\mathbf{A} \\mathbf{X}= \\mathbf{X} \\mathbf{\\Lambda}$$ $$\\mathbf{X}^{-1} \\mathbf{A} \\mathbf{X} = \\mathbf{X}^{-1} \\mathbf{X} \\mathbf{\\Lambda}$$ A matrix and its inverse colliding is like matter and anti-matter coming into contact, they both annihilate each other, albeit without the explosion. Hence: $$\\mathbf{X}^{-1} \\mathbf{A} \\mathbf{X} = \\Lambda$$ $\\Lambda$ is a diagonal matrix with the eigenvalues of $\\mathbf{A}$ on the diagonal! Hence the matrix A is diagonalized!\nWhen we want to find the powers of $\\mathbf{A}$, we can use the diagonalization equation, going the other way around: $$\\mathbf{A} \\mathbf{X}= \\mathbf{X} \\mathbf{\\Lambda}$$ $$\\mathbf{A} \\mathbf{X} \\mathbf{X^{-1}}= \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X^{-1}}$$ $$\\mathbf{A}= \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X^{-1}}$$\nSince it doesn\u0026rsquo;t matter to the eigenvectors what they\u0026rsquo;re being scaled by and since $\\Lambda$ is a diagonal matrix: $$\\mathbf{A}^k= \\mathbf{X} \\mathbf{\\Lambda}^k \\mathbf{X^{-1}}$$\nEt voil√†! We can finally do PCA!\nFinally, PCA Along the course of this journey, we only encountered one matrix, the covariance matrix. Recall that the covariance matrix will always be a square matrix, i.e. it will have the dimensions $(m,m)$. We can diagonalize the covariance matrix (remember that the covariance matrix is denoted by $\\Sigma$): $$\\mathbf{\\Sigma}= \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P^{-1}}$$ where $\\Lambda$ is a diagonal matrix with the eigenvalues of $\\mathbf{\\Sigma}$ and $\\mathbf{P}$ is a matrix with the eigenvectors of $\\mathbf{\\Sigma}$ as its columns.\nYou could also call the diagonalization of a matrix its eigenvalue decomposition, because when we diagonalize a matrix, we express it in terms of its eigenvalues and eigenvectors.\nOur premise is that we will convert $m$ dimensional data to $n$ dimensional data. Very simply, if I had a $k$ such $m$ dimensional points $(k,m)$ and I wished to project / transform to $k$ points to $n$ dimensions, I could very naively multiply the original $(k,m)$ matrix by a $(m,n)$ dimensional matrix to obtain a $(k,n)$ dimensional matrix. $$(k,m)\\cdot(m,n) \\rightarrow (k,n)$$\nTo obtain the $(m,n)$ dimensional matrix, we just need to use the covariance matrix which is $(m,m)$ dimensional.\n$$\\mathbf{\\Sigma} = \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P^{-1}}$$\nSort the eigenvalues in descending order. $$\\mathbf{\\Lambda_{\\text{sorted}}} = \\text{sort}(\\mathbf{\\Lambda}, \\text{descending})$$\nWe use these eigenvalues to then pick the top $n$ eigenvectors corresponding to the top eigenvalues. $$\\mathbf{\\Lambda_n} = \\text{diag}(\\mathbf{\\Lambda_{\\text{sorted}}[:n]})$$ $$\\mathbf{\\Lambda_n} \\rightarrow \\mathbf{P_n}$$ $$\\mathbf{P_n} = \\mathbf{P[:, :n]}$$ The notation at the end is just numpy notation for picking all rows and the first $n$ columns of the matrix $\\mathbf{P}$.\n$P_n$ is the projection matrix. We then multiply the original $(k,m)$ dimensional matrix by the $(m,n)$ dimensional projection matrix to obtain the $(k,n)$ dimensional matrix. Is this the end of the story? Are we finally done with this long long post? Do I just simply multiply the input matrix X? $$\\mathbf{X} \\mathbf{P_n} \\rightarrow (k,n)$$\n# let\u0026#39;s look at some code to answer this question: n_samples = 15 X = np.random.rand(n_samples,1) for _ in range(2, 11): X = np.hstack((X, (10**_)*np.random.rand(n_samples,1))) X.shape (15, 10)\rIn the previous cell, I created some inputs X which has n_samples number of samples and each column\u0026rsquo;s magnitude is 10 times the previous column\u0026rsquo;s magnitude.\ndef pca(X: np.array, k: int, standardize:bool=True) -\u0026gt; np.array: \u0026#34;\u0026#34;\u0026#34;Finds pca. Can select whether or not to standarize the data. Args: X (np.array): Input data. k (int): Number of principal components to retain. standarize (bool, optional): Flag to choose whether to standardize. Defaults to True. Returns: np.array: projected data \u0026#34;\u0026#34;\u0026#34; # let\u0026#39;s find the mean for each feature mean = X.mean(axis=0) # subtract the mean from each feature; shape is broadcasted if standardize: X = X - mean covariance_matrix = np.dot(X.T, X) / X.shape[0] # find eigenvalues and eigenvectors; eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix) # sort the eigenvectors by decreasing eigenvalues eigenvectors = eigenvectors[:, np.argsort(eigenvalues)[::-1]] # multiply by the input if standardize: return np.dot(X, eigenvectors[:, :k])/np.sqrt(eigenvalues[:k]) else: return np.dot(X, eigenvectors[:, :k]) In the cell above, we define a PCA function. To compute the covariance matrix, instead of using np.cov(X, rowvar=False), we use the formula for covariance just for practice. The dot product method is kinda neat, but it run in numerical instabilities (read as divide by zero errors). So overall, you might wanna stick to np.cov(X, rowvar=False). But anyway to explain the dot product - X is $(m,n)$ and we need a $(n,n)$ covariance matrix. We need to multiply every value in X by every other value in X and sum the results. This is the same as multiplying X by its transpose. The transpose of X, X.T is $(n,m)$. The dot product of transpose of X and X is $(n,m)$ x $(m,n)$ = $(n,n)$ . We divide by X.shape[0] to get the mean. Let\u0026rsquo;s try compute PCA with and without standardization:\npca(X, 3,standardize=False) array([[-7.87659032e+09, 1.21888717e+08, 9.32299737e+06],\r[-3.81517032e+09, -7.26896071e+08, 1.23143528e+07],\r[-4.20032164e+09, -5.04772750e+08, 4.93815085e+06],\r[-3.27768073e+09, -7.81313407e+07, -8.53722860e+06],\r[-5.24910853e+09, -4.10123167e+08, -3.68877997e+07],\r[-9.61861820e+09, 2.10930492e+08, 1.95338785e+07],\r[-2.44899771e+09, -2.57069015e+08, -3.33657203e+07],\r[-8.72847390e+09, 2.47534780e+08, 1.22615401e+07],\r[-4.21212718e+09, 2.84941041e+08, -5.27834809e+07],\r[-3.85027490e+09, 2.87810264e+07, -5.64613414e+07],\r[-8.99977542e+09, 6.58608889e+07, 3.38969959e+07],\r[-9.33259978e+09, -6.94629542e+06, 1.36187000e+07],\r[-1.07794112e+09, -2.85311558e+08, -4.32094387e+07],\r[-5.15673575e+09, -2.24746557e+08, 3.51379560e+07],\r[-7.50218395e+09, 3.21229490e+08, -3.46309614e+07]])\rpca(X, 3, standardize=True) array([[-0.82283229, -0.23898001, 0.34658066],\r[ 0.71682725, 2.11362558, 0.04130555],\r[ 0.56736939, 1.31895532, 0.15915883],\r[ 0.90200762, -0.64220242, 1.31329582],\r[ 0.17293128, 1.20216573, -2.24997976],\r[-1.47652018, -0.13882577, 0.21043122],\r[ 1.21636054, -0.14541439, -0.06016706],\r[-1.14450273, -0.52078503, 0.42350452],\r[ 0.54376545, -1.86936294, -0.31600902],\r[ 0.68537049, -0.93441812, -1.12269834],\r[-1.24157013, 0.28645995, 0.81769758],\r[-1.36420737, 0.66142018, -0.64117307],\r[ 1.72982373, -0.39302796, 0.09484229],\r[ 0.20275971, 0.45025174, 2.05781732],\r[-0.68758276, -1.14986187, -1.07460652]])\rObserve that we get way more amenable projected values when we choose to standardize despite having features in several different scales. PCA is sensitive to the scale of the features entered, so standardizing ensures that we do not assign higher importance to larger features. Standardizing also helps you avoid overflow errors, i.e. numbers that are too large to be represented by your selected datatype. Typically, to standardize you subtract the mean and divide by the standard deviation. $$X_{standardized} = \\frac{{X - \\mu}}{{\\sigma}}$$\nSo, finally, we express PCA as: $$ \\mathbf{Z}=\\mathbf{\\Lambda}_n^{-\\frac{1}{2}} (\\mathbf{X}-\\overline{\\mathbf{X}})\\mathbf{P}_n $$ $$ \\mathbf{Z}=\\mathbf{\\Lambda}_n^{-\\frac{1}{2}} \\mathbf{P}_n^T(\\mathbf{X}-\\overline{\\mathbf{X}}) $$ $\\mathbf{Z}$ is the projection of $\\mathbf{X}$ in the lower dimensional space. $\\mathbf{\\Lambda}_n^{-\\frac{1}{2}}$ is the inverse square root of the eigenvalues. This is where we\u0026rsquo;re dividing everything by the standard deviation. $\\mathbf{P}_n$ is the projection matrix. $\\mathbf{X}$ is the original data. $\\overline{\\mathbf{X}}$ is the mean of the original data. The two forms are equivalent, we just need to change the order based on the shape of matrix. Standard deviation is the root of the variance, so the division by the square root of the eigenvalue matrix (diagonal matrix) or multiplication by the negative half power, is the same as dividing by the standard deviation. In essence, we\u0026rsquo;re just standardizing the data for the reasons we demonstrated in code.\nWhat next? Well we just linearly transformed our data into lower dimensions. You could learn about Kernel PCA for non-linear transformations. Just a note, matrices with repeated eigenvalues cannot be diagonalized. So not all matrices can be diagonalized with this approach. I believe that most PCA implementations nowadays use SVD instead of eigenvalue decompositions since SVD can work even with matrices that have repeated eigenvalues. You could learn about SVD. References Slides by Pascal Vincet and explanations by Ioannis Mitliagkas in the course IFT630 - Fundamentals of Machine Learning taught at the University of Montreal. A tutorial on Principal Component Analysis by Lindsay I Smith at the University of Montreal. Analysis of a complex of statistical variables into principal components by Harold Hotelling. PCA - International Journal of Livestock Research. Implementing a Principal Component Analysis (PCA) - Sebastian Raschka. Introduction to Linear Algebra by Gilbert Strang. Interactive Linear Algebra by Dan Margalit and Joseph Rabinoff. The Mange Guide to Linear Algebra by Shin Takahashi and Iroha Inoue. ","permalink":"https://etrama.github.io/posts/2023-10-07-pca/","summary":"View this post in a Jupyter Notebook\nDemystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.\nUntil I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features.","title":"Demystifying the mathematics behind PCA"},{"content":"Implemented a complete Data Science pipeline: data collection, tidying data, creation of synthetic features, basic and advanced interactive visualizations using plotly, tracking models through CometML, deploying models through a REST API using Docker and Flask.\n","permalink":"https://etrama.github.io/posts/2021-12-23-nhl-ds-project/","summary":"Implemented a complete Data Science pipeline: data collection, tidying data, creation of synthetic features, basic and advanced interactive visualizations using plotly, tracking models through CometML, deploying models through a REST API using Docker and Flask.","title":"NHL Data Science Project"},{"content":"Given meteorological and satellite data, predicted land as either crop or non-crop land. Used techniques such as AutoML (Light AutoML and PyCaret) as well as blending and stacking to reduce bias and generalize better. Check out the doc link for a more detailed overview of the project.\n","permalink":"https://etrama.github.io/posts/2021-11-30-crop-harvest/","summary":"Given meteorological and satellite data, predicted land as either crop or non-crop land. Used techniques such as AutoML (Light AutoML and PyCaret) as well as blending and stacking to reduce bias and generalize better. Check out the doc link for a more detailed overview of the project.","title":"Crop Harvest Classification"},{"content":"Classified events as either standard background conditions / tropical cyclones or atmospheric rivers. Used techniques such as SMOTE and SMOTE Tomek to fix class imbalance, hyperparameter tuning using HalvingRandomGridSearchCV, manual feature engineering, and a plethora of sklearn classification algorithms. Check out the doc link for a more detailed overview of the project.\n","permalink":"https://etrama.github.io/posts/2021-10-15-weather-events/","summary":"Classified events as either standard background conditions / tropical cyclones or atmospheric rivers. Used techniques such as SMOTE and SMOTE Tomek to fix class imbalance, hyperparameter tuning using HalvingRandomGridSearchCV, manual feature engineering, and a plethora of sklearn classification algorithms. Check out the doc link for a more detailed overview of the project.","title":"Weather Events Classification"},{"content":"My current magnum opus. Consecutively scraped a ton of images for the top 100 anime followed by scraping even more than a ton of images of the top 10 characters in each anime. The goal was to try to identify anime using a ML model and when successful, we planned to identify the characters in the image as well. WIP.\n","permalink":"https://etrama.github.io/posts/2021-07-23-anime-project/","summary":"My current magnum opus. Consecutively scraped a ton of images for the top 100 anime followed by scraping even more than a ton of images of the top 10 characters in each anime. The goal was to try to identify anime using a ML model and when successful, we planned to identify the characters in the image as well. WIP.","title":"Anime Project"},{"content":"The paper provides techniques and a checklist to prevent bias from creeping into Machine Learning models. (Co-author)\n","permalink":"https://etrama.github.io/posts/2021-01-30-whitepaper-ethics/","summary":"The paper provides techniques and a checklist to prevent bias from creeping into Machine Learning models. (Co-author)","title":"Whitepaper: Ethically mitigating biases in DS / ML"},{"content":"This challenge was a competition hosted within Deloitte, on a platform called DLabs with cash prizes for podium finishes. It was a Kaggle style competition with public and private leaderboards. Our team managed to place Second.\n","permalink":"https://etrama.github.io/posts/2020-10-30-datarobot-ideation/","summary":"This challenge was a competition hosted within Deloitte, on a platform called DLabs with cash prizes for podium finishes. It was a Kaggle style competition with public and private leaderboards. Our team managed to place Second.","title":"DataRobot Ideation Challenge"},{"content":"View this project on Github\nThis challenge was a competition hosted within Deloitte, on a platform called DLabs with cash prizes for podium finishes. It was a Kaggle style competition with public and private leaderboards. Our team managed to place Third.\nI worked on this competition with Navaneesh Gangala. I was responsible for cleaning data, validating the integrity of the cleaned data, engineering features, training models, using AutoML and ensembling models, etc.\n","permalink":"https://etrama.github.io/posts/2020-07-11-zomato-ratings/","summary":"View this project on Github\nThis challenge was a competition hosted within Deloitte, on a platform called DLabs with cash prizes for podium finishes. It was a Kaggle style competition with public and private leaderboards. Our team managed to place Third.\nI worked on this competition with Navaneesh Gangala. I was responsible for cleaning data, validating the integrity of the cleaned data, engineering features, training models, using AutoML and ensembling models, etc.","title":"Zomato Ratings Prediction Challenge"},{"content":"View this project on Github\nMy notes on a few Deep Learning papers. Hopefully, I improve my understanding of Deep Learning in the process (also helps when I want to revisit the paper) while making it easier for other people to read papers.\n","permalink":"https://etrama.github.io/posts/2019-09-22-dl-paper-notes/","summary":"View this project on Github\nMy notes on a few Deep Learning papers. Hopefully, I improve my understanding of Deep Learning in the process (also helps when I want to revisit the paper) while making it easier for other people to read papers.","title":"DL Paper Notes"},{"content":"Contributed to a guide to help resolve Jupyter Notebook connection for the FAST AI library being run on Paperspace. The medium article on the same topic has over 5k views. You can read the article here.\n","permalink":"https://etrama.github.io/posts/2018-10-17-fastai-contribution/","summary":"Contributed to a guide to help resolve Jupyter Notebook connection for the FAST AI library being run on Paperspace. The medium article on the same topic has over 5k views. You can read the article here.","title":"FastAI Contribution"},{"content":"TL;DR (Too Long; Didn\u0026rsquo;t Read) The paper addresses the following topics:\nAddressed the problem of silo\u0026rsquo;ing of data. Addressed how we could update / upgrade Machine Learning models on the fly to accomodate data drift. Discussed how to develop a new kind of data scientist who would possess both technical and business skills. Discussed how this new kind of data scientist would fit into and be aided by Agile pods. This approach would be quicker than waiting for a Data Scientist to develop business skills or vice-versa. Background Thanks to our team winning the 2018 Deloitte ML Guild Hackathon, I was invited to be a part of a panel of authors with the goal of writing a Whitepaper. The authors on this whitepaper other than myself include: Tami Frankenfeld, Kyle Harbacek, Arunima Gupta, Abhishek Dugar, Sharad Kumar and Devin Cavagnaro.\nThe paper was published on Deloitte‚Äôs internal knowledge sharing platform, called KX. The paper was the most widely read whitepaper on the platform in 2018.\nWhat is the Dataspace in ML? We identified 3 challenges extant in the ML / DS space, which we call the \u0026ldquo;Dataspace\u0026rdquo;. The challenges are:\nData in an organization is not always centralized. It is often siloed in different departments, and even within departments, it is siloed in different teams. When ML models are deployed, they are often not monitored for performance. Models might not be making predictions on the same data that they were trained on, due to \u0026ldquo;data drift\u0026rdquo; and the model itself influencing future data perhaps. The people, process and technology changes in an organization that are required to implement ML are not always well understood. How do we address this Dataspace? While I cannot go into much detail due to confidentiality agreements, if you are a firm that wishes to contract Deloitte to help you with your ML journey, you can contact the authors of this paper who are still working at Deloitte.\nMy contribution was a section on the people aspect of the Dataspace. We came up with a career roadmap for people with business skills who are interested in ML, and vice-versa. This methodology was combined with Agile pods. With this combination, we were able to chart a course for people at any career level to develop both the required technical acumen and business skills to be successful in their chosen business domain.\n","permalink":"https://etrama.github.io/posts/2018-09-15-whitepaper-dataspace/","summary":"TL;DR (Too Long; Didn\u0026rsquo;t Read) The paper addresses the following topics:\nAddressed the problem of silo\u0026rsquo;ing of data. Addressed how we could update / upgrade Machine Learning models on the fly to accomodate data drift. Discussed how to develop a new kind of data scientist who would possess both technical and business skills. Discussed how this new kind of data scientist would fit into and be aided by Agile pods. This approach would be quicker than waiting for a Data Scientist to develop business skills or vice-versa.","title":"Whitepaper: Bridging the Dataspace in ML"},{"content":"View this project on Github\nBackground Every year the Deloitte Machine Learning Guild hosts an their annual hackathon, which is the largest Data Science / ML hackathon of its kind within Deloitte. Interested folks form teams of 2-4 people and work on the assigned hackathon theme for a duration of 2-4 weeks, after which the teams are judged based on a variety of criteria. The judges are Deloite ML Guild Masters who are subject matter experts in ML and DS, as well as partners who are interested in the field. The winning team gets a cash prize (USD 500 each) and a chance to present their work to the entire ML Guild.\nTeam I was part of a team of 5 people, including myself. The other members were Rohit Shah, Swapna Batta, Milonee Mehta and Austin Lasseter. We participated and placed First in the 2018 version of the Annual Deloitte ML Guild Hackathon.\nIdea Access to fresh, healthy food plays a big role in determining whether the population of a given area is healthy or not. In the US, there are many areas where access to fresh food is limited, and the population is forced to rely on fast food and other unhealthy options. This leads to a variety of health issues, including obesity, diabetes, heart disease, etc.\nWe wanted to use ML to help local governments decide where to provide benefits and subsidies to open new grocery stores. Governments could also decide to increase SNAP (food stamps) benefits for the people in these areas, or undertake some other measure to address the problem.\nOur work is based on research conducted by Alana Rhone, who I believe is also the one who graciously made this data public.\nData We used publicly available data, the USDA Food Environment Atlas to predict which counties in the US are most in need of a grocery store, and then use this information to help local governments decide where to provide benefits and subsidies to open new grocery stores.\nApproach Based on health based factors such as BMI and the incidence of diabetes, we created a predictive health dashboard that predicts which counties in the US are most in need of a grocery store. We used a variety of ML models, including Random Forests, XGBoost, and Neural Networks to predict the health of a county based on the food environment in that county. We also used Plotly to create an interactive dashboard that allows users to explore the data and see the predictions for each county.\nWe came up with a small matrix based the predictions of our models and the actual health of the county, and used this matrix to rank the counties in the US based on their need for a grocery store. This matrix is based on two different models that we trained, a model which predicts whether the number of grocery stores in the a certain country will increase or decrease and another model predicts whether the diabetes rate in a county will increase or decrease.\nBased on these two models, we create a matrix as follows:\nGrocery Stores Diabetes Rate Recommendation Decrease Decrease Monitor concentration of stores \u0026amp; population served Decrease Increase Open more stores / provide more SNAP benefits Increase Decrease Ideal Increase Increase Monitor concentration of stores \u0026amp; population served We then used this ranking to create a map of the US, where the counties are colored based on their need for a grocery store. This map can be used by local governments to decide where to provide benefits and subsidies to open new grocery stores. One major feature that both we and the judges seemed to like about our approach is that is uses no PII / PHI (Personally Identifiable Information / Person Health Information) data about the people themselves.\nFuture Work I want to check whether I can somehow add to the analysis performed by Alana and her colleagues in their work. I want to redo this project from scratch, and check how my current skills compare to my skills when I first did this project. Presentation is everything. I want to redo the dash app where this was deployed and figure out if there\u0026rsquo;s a way to keep it running without having to pay for it. Maybe another github.io page? Are there any drawbacks to combining the predictions of the two models like this? Does it matter whether they are trained on the same data or different data? Ulterior Motives Everytime I have to prepare for an interview, I need to go through the repo that we made for this project, which is a headache. This page helps me ease that process. Additionally, I can use this as a resource to present in case I talk about this project during my interviews, especially if I make the app live again.\n","permalink":"https://etrama.github.io/posts/2018-08-30-guild-hackathon/","summary":"View this project on Github\nBackground Every year the Deloitte Machine Learning Guild hosts an their annual hackathon, which is the largest Data Science / ML hackathon of its kind within Deloitte. Interested folks form teams of 2-4 people and work on the assigned hackathon theme for a duration of 2-4 weeks, after which the teams are judged based on a variety of criteria. The judges are Deloite ML Guild Masters who are subject matter experts in ML and DS, as well as partners who are interested in the field.","title":"Deloitte 2018 ML Guild Hackathon"},{"content":"This post is a collection of projects I have worked on at Deloitte. These aren\u0026rsquo;t related to Machine Learning per se. The ML projects at Deloitte are tagged as Deloitte in their respective posts.\nOracle Security and Controls: Implemented Oracle EBS and Oracle Cloud Security and Controls on FSCM (Financial Supply Chain Management) and HCM (Human Capital Management) modules on multiple client engagements. Responsible for role builds, defect resolution, data security and access provisioning. Worked to implement the principles of RBAC (Role based Access Control) and SOD (Segregation of Duties). My work at Deloitte also encompasses: Machine Learning RPA (Robotic Process Automation) Python Scripts for automation and data cleaning Kubernetes Prisma Cloud Compute VBA (Visual Basic for Applications) scripting. ","permalink":"https://etrama.github.io/posts/2018-06-10-deloitte-projects/","summary":"This post is a collection of projects I have worked on at Deloitte. These aren\u0026rsquo;t related to Machine Learning per se. The ML projects at Deloitte are tagged as Deloitte in their respective posts.\nOracle Security and Controls: Implemented Oracle EBS and Oracle Cloud Security and Controls on FSCM (Financial Supply Chain Management) and HCM (Human Capital Management) modules on multiple client engagements. Responsible for role builds, defect resolution, data security and access provisioning.","title":"Projects @ Deloitte"},{"content":"Some random projects that I have worked on: Excel_editor - Python script to automate and simplify common Excel tasks 9gag_scraper - Python script to scrape 9gag gifmymovies - Project to convert songs into gifs Scraping-Cleaning-Threading - Multithreaded scraping in Python Machine Learning for Book Recommendation Systems ML-Insurance-Competition - Prediction of Valid Insurance Claims Pork Price Prediction IMDB Sentiment Review ","permalink":"https://etrama.github.io/posts/2016-12-10-misc-projects/","summary":"Some random projects that I have worked on: Excel_editor - Python script to automate and simplify common Excel tasks 9gag_scraper - Python script to scrape 9gag gifmymovies - Project to convert songs into gifs Scraping-Cleaning-Threading - Multithreaded scraping in Python Machine Learning for Book Recommendation Systems ML-Insurance-Competition - Prediction of Valid Insurance Claims Pork Price Prediction IMDB Sentiment Review ","title":"Miscellaneous Projects"},{"content":"Education I studied at the Manipal Institute of Technology, Manipal, India. I graduated with a Bachelor of Technology in Electronics and Communication Engineering in 2017.\nWhen I joined I was enrolled in the Chemical Engineering program. I switched to Electronics and Communication Engineering in my second year. I graduated with a CGPA of 8.6/10.\nElectives and Projects Electives Shell Scripting in Linux Cyptography MEMS (micro electro mechanical systems) Robotics. Projects Implemented AES (Advanced Encryption Standard) from scratch in VHDL Made a digit recognizer using the MSP420 micro-controller Built a chainsaw robot for TechTatva Built a heat exchanger ","permalink":"https://etrama.github.io/posts/2016-07-10-undergrad-projects/","summary":"Education I studied at the Manipal Institute of Technology, Manipal, India. I graduated with a Bachelor of Technology in Electronics and Communication Engineering in 2017.\nWhen I joined I was enrolled in the Chemical Engineering program. I switched to Electronics and Communication Engineering in my second year. I graduated with a CGPA of 8.6/10.\nElectives and Projects Electives Shell Scripting in Linux Cyptography MEMS (micro electro mechanical systems) Robotics. Projects Implemented AES (Advanced Encryption Standard) from scratch in VHDL Made a digit recognizer using the MSP420 micro-controller Built a chainsaw robot for TechTatva Built a heat exchanger ","title":"Undergrad Projects"}]