<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Demystifying the mathematics behind PCA | Old Habits AI Hard</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta name=keywords content="Dimensionality Reduction,PCA,Machine Learning,Linear Algebra"><meta name=description content="View this post in a Jupyter Notebook
Demystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.
Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features."><meta name=author content="Kaushik Moudgalya"><link rel=canonical href=https://etrama.github.io/posts/2023-10-07-pca/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://etrama.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://etrama.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://etrama.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://etrama.github.io/apple-touch-icon.png><link rel=mask-icon href=https://etrama.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZQRQFZ413J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZQRQFZ413J",{anonymize_ip:!1})}</script><meta property="og:title" content="Demystifying the mathematics behind PCA"><meta property="og:description" content="View this post in a Jupyter Notebook
Demystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.
Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features."><meta property="og:type" content="article"><meta property="og:url" content="https://etrama.github.io/posts/2023-10-07-pca/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-07T18:36:11-04:00"><meta property="article:modified_time" content="2023-10-07T18:36:11-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Demystifying the mathematics behind PCA"><meta name=twitter:description content="View this post in a Jupyter Notebook
Demystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.
Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://etrama.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Demystifying the mathematics behind PCA","item":"https://etrama.github.io/posts/2023-10-07-pca/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Demystifying the mathematics behind PCA","name":"Demystifying the mathematics behind PCA","description":"View this post in a Jupyter Notebook\nDemystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.\nUntil I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features.","keywords":["Dimensionality Reduction","PCA","Machine Learning","Linear Algebra"],"articleBody":"View this post in a Jupyter Notebook\nDemystifying the mathematics behind PCA We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used PCA. I thought I knew PCA.\nUntil I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features. The interviewer was even kind enough to throw me a hint about projections. To my chagrin, I couldn’t figure it out even then and had to admit I was stumped. I knew that I was taught the mathematics behind PCA during the first year of my Master’s course, which made it feel worse. So here’s a post to make sure that doesn’t happen to you, dear reader, and hopefully me as well 😆.\nWARNING: This post will be long and mathematical as the title suggests.\nIntroducing PCA PCA stands for Principal Component Analysis. If you have multidimensional data that’s giving you a hard time when you try to train a model on it, PCA could be the answer. You could also visualize high dimensional data using PCA, which is done often in NLP.\n# there's no understanding without doing, so let's write some code import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as mcolors import seaborn as sns import random sns.set_style(\"darkgrid\") from sklearn.datasets import load_wine np.random.seed(420) We have a set of data, that we will call $x$. $x$ is $(k,m)$ dimensional, hence every point in $x$ will be such that $x_i \\in \\mathbb{R}^m$ (just a fancy way of saying we pick a random point from x and that point is a real number in $m$ dimensions). We have $k$ such $m$ dimensional points. Our goal is to represent $x$ in less than $m$ dimensions. This is by no means going to be a perfect transition. Say we project down from $m$ dimensions to $n$ dimensions. By definition, $n","wordCount":"4696","inLanguage":"en","datePublished":"2023-10-07T18:36:11-04:00","dateModified":"2023-10-07T18:36:11-04:00","author":{"@type":"Person","name":"Kaushik Moudgalya"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://etrama.github.io/posts/2023-10-07-pca/"},"publisher":{"@type":"Organization","name":"Old Habits AI Hard","logo":{"@type":"ImageObject","url":"https://etrama.github.io/favicon.ico"}}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-ZQRQFZ413J"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-ZQRQFZ413J")</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://etrama.github.io/ accesskey=h title="Old Habits AI Hard (Alt + H)">Old Habits AI Hard</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://etrama.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://etrama.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://etrama.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://etrama.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://etrama.github.io/ title=Home><span>Home</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://etrama.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://etrama.github.io/posts/>Posts</a></div><h1 class=post-title>Demystifying the mathematics behind PCA</h1><div class=post-meta><span title='2023-10-07 18:36:11 -0400 EDT'>October 7, 2023</span>&nbsp;·&nbsp;23 min&nbsp;·&nbsp;Kaushik Moudgalya</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#demystifying-the-mathematics-behind-pca>Demystifying the mathematics behind PCA</a></li><li><a href=#introducing-pca>Introducing PCA</a></li><li><a href=#gentle-math>Gentle math</a><ul><li><a href=#standard-deviation>Standard deviation</a></li><li><a href=#variance>Variance</a></li><li><a href=#covariance>Covariance</a></li><li><a href=#proof-that-the-covariance-matrix-is-symmetric>Proof that the covariance matrix is symmetric</a></li></ul></li><li><a href=#serious-math>Serious math</a><ul><li><a href=#eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors</a></li><li><a href=#eigenvalues-visualized>Eigenvalues visualized</a></li><li><a href=#eigenvalues-generalized>Eigenvalues generalized</a></li></ul></li><li><a href=#powers-of-a-matrix>Powers of a matrix</a><ul><li><a href=#diagonal-matrices-and-eigenvalues>Diagonal matrices and eigenvalues</a></li></ul></li><li><a href=#finally-pca>Finally, PCA</a></li><li><a href=#what-next>What next?</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p><a href=https://github.com/Etrama/statistics-primer/blob/main/src/001-PCA.ipynb>View this post in a Jupyter Notebook</a></p><h2 id=demystifying-the-mathematics-behind-pca>Demystifying the mathematics behind PCA<a hidden class=anchor aria-hidden=true href=#demystifying-the-mathematics-behind-pca>#</a></h2><p>We all know PCA and we all love PCA. Our friend that helps us deal with the curse of dimensionality. All data scientists have probably used <a href=https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>PCA</a>. I thought I knew PCA.</p><p>Until I was asked to explain the mathematics behind PCA in an interview and all I could murmur was that it somehow maximizes the variance of the new features. The interviewer was even kind enough to throw me a hint about projections. To my chagrin, I couldn&rsquo;t figure it out even then and had to admit I was stumped. I knew that I was taught the mathematics behind PCA during the first year of my Master&rsquo;s course, which made it feel worse. So here&rsquo;s a post to make sure that doesn&rsquo;t happen to you, dear reader, and hopefully me as well 😆.</p><p>WARNING: This post will be long and mathematical as the title suggests.</p><h2 id=introducing-pca>Introducing PCA<a hidden class=anchor aria-hidden=true href=#introducing-pca>#</a></h2><p>PCA stands for Principal Component Analysis. If you have multidimensional data that&rsquo;s giving you a hard time when you try to train a model on it, PCA could be the answer. You could also visualize high dimensional data using PCA, which is done often in <a href=https://necromuralist.github.io/Neurotic-Networking/posts/nlp/pca-dimensionality-reduction-and-word-vectors/index.html>NLP</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># there&#39;s no understanding without doing, so let&#39;s write some code</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.colors <span style=color:#66d9ef>as</span> mcolors
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>set_style(<span style=color:#e6db74>&#34;darkgrid&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_wine
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>420</span>)
</span></span></code></pre></div><p>We have a set of data, that we will call $x$. $x$ is $(k,m)$ dimensional, hence every point in $x$ will be such that $x_i \in \mathbb{R}^m$ (just a fancy way of saying we pick a random point from x and that point is a real number in $m$ dimensions). We have $k$ such $m$ dimensional points. Our goal is to represent $x$ in less than $m$ dimensions. This is by no means going to be a perfect transition. Say we project down from $m$ dimensions to $n$ dimensions. By definition, $n&lt;m$.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># say D=10, i.e we have 10 dimensional data.</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># for the geeks amongst us this is from a continuous uniform distrbution from [0, 1)</span>
</span></span><span style=display:flex><span>x<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e># so we have 10 (k=10) data points each of which have 10 dimensions (m=10)</span>
</span></span></code></pre></div><pre><code>(10, 10)
</code></pre><p>The typical approach is to maximize the variance of the $n$ dimensional data in such a way that it captures as much of the variance of the $m$ dimensional data as possible. Let&rsquo;s call $z$ the co-ordinates of the projection of $x$ into the lower $n$ dimensional space.</p><p><a href=https://en.wikipedia.org/wiki/Harold_Hotelling>Harold Hotelling</a> is credited with coming up with the idea of minimizing the variance in 1933. Mr.Hotelling was quite the madlad, since he also came up with the T-square distribution amongst other laws, lemmas and rules. He wrote presciently in his original paper, 90 years ago:</p><blockquote><p>&ldquo;It is natural to ask whether some more fundamental set of independent variables exists, perhaps fewer in number than the $x$&rsquo;s, which determine the values the $x$&rsquo;s will take.&rdquo;</p></blockquote><h2 id=gentle-math>Gentle math<a hidden class=anchor aria-hidden=true href=#gentle-math>#</a></h2><p>Let&rsquo;s introduce the mean. This is our &ldquo;average&rdquo; friend.
$$\bar{x}=\frac{1}{n}\sum_{i}^{n}x_i$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># in numpy if we don&#39;t define the axis</span>
</span></span><span style=display:flex><span><span style=color:#75715e># it will compute the mean of all the 100 elements that we entered</span>
</span></span><span style=display:flex><span>x<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><pre><code>0.48955083684170964
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># since we&#39;re dealing with components,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># we are more interested in the columwise or featurewise mean</span>
</span></span><span style=display:flex><span>x<span style=color:#f92672>.</span>mean(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><pre><code>array([0.50299989, 0.46627609, 0.62990129, 0.5188987 , 0.43764572,
       0.59755813, 0.48329069, 0.47277132, 0.46266469, 0.32350185])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># if you&#39;re paranoid and you&#39;d like to check the values, here you go:</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    print(x[:,i]<span style=color:#f92672>.</span>sum()<span style=color:#f92672>/</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><pre><code>0.5029998856345316
0.4662760922130609
0.6299012923571106
0.5188986993892943
0.4376457212611754
0.5975581266463694
0.48329069068570707
0.47277132022054413
0.46266468509761466
0.3235018549116885
</code></pre><h3 id=standard-deviation>Standard deviation<a hidden class=anchor aria-hidden=true href=#standard-deviation>#</a></h3><p>The standard deviation is just how much each point deviates from the mean. It measures how spread out our data is. Standard deviation is denoted as $\sigma$.</p><p>$$\sigma=\sqrt{\frac{1}{n}\sum_{i}^{n}(x_i - \bar{x})^2}$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x<span style=color:#f92672>.</span>std(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><pre><code>array([0.25282779, 0.31208093, 0.24974188, 0.31661173, 0.30802279,
       0.25662288, 0.1512335 , 0.22761326, 0.22715909, 0.3097734 ])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># say we have some different data:</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>0</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span><span style=color:#f92672>+</span>[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>7</span>)
</span></span><span style=display:flex><span>arrays <span style=color:#f92672>=</span> [y, z, w]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># we can check their means:</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> arrays:
</span></span><span style=display:flex><span>    print(_<span style=color:#f92672>.</span>mean())
</span></span></code></pre></div><pre><code>1.0
0.5
3.5
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># we can also check their standard deviation:</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> arrays:
</span></span><span style=display:flex><span>    print(_<span style=color:#f92672>.</span>std())
</span></span></code></pre></div><pre><code>0.0
0.5
1.707825127659933
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># paranoia tax</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> arrays:
</span></span><span style=display:flex><span>    current_mean <span style=color:#f92672>=</span> _<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    print(np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>sum((_ <span style=color:#f92672>-</span> current_mean)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>/</span>len(_)))
</span></span></code></pre></div><pre><code>0.0
0.5
1.707825127659933
</code></pre><p>This makes sense when you consider that our array y had only one value and has a standard deviation of 0. Our array z was slightly more spread out, but still it only had 2 unique values each occuring thrice. Our array w was the most numerically diverse of y, z and w, and hence it had the highest standard deviation.</p><h3 id=variance>Variance<a hidden class=anchor aria-hidden=true href=#variance>#</a></h3><p>The variance is simply the square of the standard deviation.</p><p>$$ var(x) = \sigma^2=\frac{1}{n}\sum_{i}^{n}(x_i - \bar{x})^2$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x<span style=color:#f92672>.</span>var(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><pre><code>array([0.06392189, 0.09739451, 0.06237101, 0.10024299, 0.09487804,
       0.06585531, 0.02287157, 0.0518078 , 0.05160125, 0.09595956])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># consider the 1d line data again</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> arrays:
</span></span><span style=display:flex><span>    print(_<span style=color:#f92672>.</span>var())
</span></span></code></pre></div><pre><code>0.0
0.25
2.9166666666666665
</code></pre><p>All the measures we looked at so far were dealing with only one dimension. Notice that we have just one number to describe the mean, $\sigma$ and $\sigma^2$ for our one-liner data (y,z,w), since they contain only one dimension. However, we had to specify an axis for $x$, since $x$ is a collection of 10 data points with 10 dimensions each. Also notice that when dealing with $x$, all 3 measures had 10 values, that is as many values as the number of dimensions in $x$.</p><h3 id=covariance>Covariance<a hidden class=anchor aria-hidden=true href=#covariance>#</a></h3><p>To figure out the interactions between the dimensions and how they vary depending on each other, we introduce the aptly named covariance term. The covariance is always measured between two dimensions. Notice the similarities between the variance and the covariance below:</p><p>$$ var(x) \frac{1}{n}\sum_{i}^{n}(x_i - \bar{x})(x_i - \bar{x})$$</p><p>$$ cov(x,y) = \frac{1}{n}\sum_{i}^{n}(x_i - \bar{x})(y_i - \bar{y})$$</p><p>It might be nice to see the covariance of variable on a real dataset rather than the random data we have here:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># toy data from the wine sklearn dataset</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_wine
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> load_wine()[<span style=color:#e6db74>&#34;data&#34;</span>]
</span></span><span style=display:flex><span>data<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e># we have 13 features</span>
</span></span></code></pre></div><pre><code>(178, 13)
</code></pre><p>Covariance can only capture the variance between 2 variables. Additionally, we also know that the covarince of a feature with itself is the variance. As far as a count of covariances goes, in this case with 13 features, we will have a 13x13 matrix, since there 12 other features to compare with and one of the feature compared with itself (variance). All of the above mental gymnastics just allows us to express covariance a matrix. The covariance matrix is usually denoted by $\Sigma$.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># features are in columns, hence rowvar is false</span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>cov(data, rowvar<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><pre><code>(13, 13)
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>np<span style=color:#f92672>.</span>cov(data, rowvar<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre><code>array([[ 6.59062328e-01,  8.56113090e-02,  4.71151590e-02,
        -8.41092903e-01,  3.13987812e+00,  1.46887218e-01,
         1.92033222e-01, -1.57542595e-02,  6.35175205e-02,
         1.02828254e+00, -1.33134432e-02,  4.16978226e-02,
         1.64567185e+02],
       [ 8.56113090e-02,  1.24801540e+00,  5.02770393e-02,
         1.07633171e+00, -8.70779534e-01, -2.34337723e-01,
        -4.58630366e-01,  4.07333619e-02, -1.41146982e-01,
         6.44838183e-01, -1.43325638e-01, -2.92447483e-01,
        -6.75488666e+01],
       [ 4.71151590e-02,  5.02770393e-02,  7.52646353e-02,
         4.06208278e-01,  1.12293658e+00,  2.21455913e-02,
         3.15347299e-02,  6.35847140e-03,  1.51557799e-03,
         1.64654327e-01, -4.68215451e-03,  7.61835841e-04,
         1.93197391e+01],
       [-8.41092903e-01,  1.07633171e+00,  4.06208278e-01,
         1.11526862e+01, -3.97476036e+00, -6.71149146e-01,
        -1.17208281e+00,  1.50421856e-01, -3.77176220e-01,
         1.45024186e-01, -2.09118054e-01, -6.56234368e-01,
        -4.63355345e+02],
       [ 3.13987812e+00, -8.70779534e-01,  1.12293658e+00,
        -3.97476036e+00,  2.03989335e+02,  1.91646988e+00,
         2.79308703e+00, -4.55563385e-01,  1.93283248e+00,
         6.62052061e+00,  1.80851266e-01,  6.69308068e-01,
         1.76915870e+03],
       [ 1.46887218e-01, -2.34337723e-01,  2.21455913e-02,
        -6.71149146e-01,  1.91646988e+00,  3.91689535e-01,
         5.40470422e-01, -3.50451247e-02,  2.19373345e-01,
        -7.99975192e-02,  6.20388758e-02,  3.11021278e-01,
         9.81710573e+01],
       [ 1.92033222e-01, -4.58630366e-01,  3.15347299e-02,
        -1.17208281e+00,  2.79308703e+00,  5.40470422e-01,
         9.97718673e-01, -6.68669999e-02,  3.73147553e-01,
        -3.99168626e-01,  1.24081969e-01,  5.58262255e-01,
         1.55447492e+02],
       [-1.57542595e-02,  4.07333619e-02,  6.35847140e-03,
         1.50421856e-01, -4.55563385e-01, -3.50451247e-02,
        -6.68669999e-02,  1.54886339e-02, -2.60598680e-02,
         4.01205097e-02, -7.47117692e-03, -4.44692440e-02,
        -1.22035863e+01],
       [ 6.35175205e-02, -1.41146982e-01,  1.51557799e-03,
        -3.77176220e-01,  1.93283248e+00,  2.19373345e-01,
         3.73147553e-01, -2.60598680e-02,  3.27594668e-01,
        -3.35039177e-02,  3.86645655e-02,  2.10932940e-01,
         5.95543338e+01],
       [ 1.02828254e+00,  6.44838183e-01,  1.64654327e-01,
         1.45024186e-01,  6.62052061e+00, -7.99975192e-02,
        -3.99168626e-01,  4.01205097e-02, -3.35039177e-02,
         5.37444938e+00, -2.76505801e-01, -7.05812576e-01,
         2.30767480e+02],
       [-1.33134432e-02, -1.43325638e-01, -4.68215451e-03,
        -2.09118054e-01,  1.80851266e-01,  6.20388758e-02,
         1.24081969e-01, -7.47117692e-03,  3.86645655e-02,
        -2.76505801e-01,  5.22449607e-02,  9.17662439e-02,
         1.70002234e+01],
       [ 4.16978226e-02, -2.92447483e-01,  7.61835841e-04,
        -6.56234368e-01,  6.69308068e-01,  3.11021278e-01,
         5.58262255e-01, -4.44692440e-02,  2.10932940e-01,
        -7.05812576e-01,  9.17662439e-02,  5.04086409e-01,
         6.99275256e+01],
       [ 1.64567185e+02, -6.75488666e+01,  1.93197391e+01,
        -4.63355345e+02,  1.76915870e+03,  9.81710573e+01,
         1.55447492e+02, -1.22035863e+01,  5.95543338e+01,
         2.30767480e+02,  1.70002234e+01,  6.99275256e+01,
         9.91667174e+04]])
</code></pre><p>Positive covariance values between two features indicate that as one feature increases the other also increases. Negative values indicate an increase-decrease relation.</p><p>The 13x13 matrix might be a little hard to visualize, so let&rsquo;s look at a covariance matrix with just 4 features, $a, b, c, d$:
$$
\Sigma = \begin{bmatrix}
cov(a,a) & cov(a,b) & cov(a,c) & cov(a,d) \\
cov(b,a) & cov(b,b) & cov(b,c) & cov(b,d) \\
cov(c,a) & cov(c,b) & cov(c,c) & cov(c,d) \\
cov(d,a) & cov(d,b) & cov(d,c) & cov(d,d) \
\end{bmatrix}
$$</p><h3 id=proof-that-the-covariance-matrix-is-symmetric>Proof that the covariance matrix is symmetric<a hidden class=anchor aria-hidden=true href=#proof-that-the-covariance-matrix-is-symmetric>#</a></h3><p>$$cov(a,b) = cov(b,a)$$
This is quite easy to prove, consider:
$$ cov(a,b) = \frac{1}{n}\sum_{i}^{n}(a_i - \bar{a})(b_i - \bar{b})$$
$$ cov(b,a) = \frac{1}{n}\sum_{i}^{n}(b_i - \bar{b})(a_i - \bar{a})$$
We know that multiplication is commutative, i.e. $ab = ba$. This makes the covariance matrix a symmetric matrix.😁</p><h2 id=serious-math>Serious math<a hidden class=anchor aria-hidden=true href=#serious-math>#</a></h2><p>Let&rsquo;s talk about matrices and their products. We can multiple two matrices $\mathbf{A}$ with dimensions (x,y) and $\mathbf{B}$, with dimensions (u,v), if and only if, y = u, i.e. the number of columns in $\mathbf{A}$ is the same as the number of rows in $\mathbf{B}$. The matrix product itself in this case will be (x,v) dimensional.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># let&#39;s say I have two matrices and I try to multiply them:</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>a<span style=color:#f92672>.</span>dot(b)<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><pre><code>(10, 20)
</code></pre><p>Every linear transformation $T: \mathbb{R}^m \rightarrow \mathbb{R}^n$ can be represented as matrix. Going from $m$ to $n$ dimensions? I bet that sounds kinda familiar. PCA is nothing but a linear transformation.</p><h3 id=eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors<a hidden class=anchor aria-hidden=true href=#eigenvalues-and-eigenvectors>#</a></h3><p>Say we have a vector $x$ and a matrix $\mathbf{A}$. Almost all vectors change direction when multipled by a matrix, i.e. they point in a different direction. When this multiplication with a matrix does not change the direction of $x$, then $x$ is called an eigenvector.
$$\mathbf{A}x = \lambda x$$
$\lambda$ is the eigenvalue of the matrix $\mathbf{A}$. While $\mathbf{A}$ may not change the direction of x, it does change the magnitude. The eigenvalue $\lambda$ tells us whether $\mathbf{A}$ increased / decreased / did neither vis-a-vis the magnitude of the eigenvector $x$.</p><h3 id=eigenvalues-visualized>Eigenvalues visualized<a hidden class=anchor aria-hidden=true href=#eigenvalues-visualized>#</a></h3><p>Right now, we kind of understand when we can multiply two matrices. The same principles apply when we&rsquo;re multiplying a matrix with a vector.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># for instance:</span>
</span></span><span style=display:flex><span>matrix1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>vector1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># we can multiply matrix1 and vector1 because: (3x3)x(3x1), </span>
</span></span><span style=display:flex><span><span style=color:#75715e># both the inner dimensions are the same just as in the</span>
</span></span><span style=display:flex><span><span style=color:#75715e># matrix to matrix multiplication</span>
</span></span><span style=display:flex><span>matrix1<span style=color:#f92672>.</span>dot(vector1)
</span></span></code></pre></div><pre><code>array([[0.52496104],
       [1.01737068],
       [1.03492964]])
</code></pre><p>I found I daresay my favorite example and demonstration of this concept in book called &ldquo;The Manga Guide to Linear Algebra&rdquo;. With thanks, I&rsquo;ll be using the same example as the book.<br>We know that we can multiply vectors and matrices under a certain condition on their dimensions. But what happens when we do multiply a matrix and a vector?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Consider the following two vectors:</span>
</span></span><span style=display:flex><span>vector1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>]) <span style=color:#75715e># represents the point, x=3, y=1</span>
</span></span><span style=display:flex><span>vector2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>]) <span style=color:#75715e># represents the point x=1, y=2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>np<span style=color:#f92672>.</span>array([vector1, vector2])
</span></span></code></pre></div><pre><code>array([[3, 1],
       [1, 2]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_vectors</span>(list_of_vectors, plot_title):
</span></span><span style=display:flex><span>    <span style=color:#75715e># https://saturncloud.io/blog/how-to-plot-vectors-in-python-using-matplotlib-a-guide-for-data-scientists/</span>
</span></span><span style=display:flex><span>    fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots()
</span></span><span style=display:flex><span>    <span style=color:#75715e># colors = list(&#34;bgrcmy&#34;)</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> vector <span style=color:#f92672>in</span> list_of_vectors:
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>quiver(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, vector[<span style=color:#ae81ff>0</span>], vector[<span style=color:#ae81ff>1</span>], angles<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;xy&#39;</span>, scale_units<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;xy&#39;</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, color<span style=color:#f92672>=</span>random<span style=color:#f92672>.</span>choice(list(mcolors<span style=color:#f92672>.</span>TABLEAU_COLORS<span style=color:#f92672>.</span>values())
</span></span><span style=display:flex><span>))
</span></span><span style=display:flex><span>    xlim <span style=color:#f92672>=</span> max(np<span style=color:#f92672>.</span>array(list_of_vectors)<span style=color:#f92672>.</span>max(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>    ylim <span style=color:#f92672>=</span> max(np<span style=color:#f92672>.</span>array(list_of_vectors)<span style=color:#f92672>.</span>max(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    ax<span style=color:#f92672>.</span>set_xlim([<span style=color:#f92672>-</span>xlim<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, xlim<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    ax<span style=color:#f92672>.</span>set_ylim([<span style=color:#f92672>-</span>ylim<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, ylim<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    <span style=color:#75715e># plt.grid()</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;x axis&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;y axis&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Figure: </span><span style=color:#e6db74>{</span>plot_title<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plot_vectors([vector1, vector2], <span style=color:#e6db74>&#34;Initial Vectors&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=./vectors_before_transformation_matplotlib.png alt="Vectors before transformation"></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># say we multiply these vectors by a 2x2 matrix</span>
</span></span><span style=display:flex><span>matrix1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([[<span style=color:#ae81ff>8</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>],[<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>matrix1
</span></span></code></pre></div><pre><code>array([[ 8, -3],
       [ 2,  1]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transformed_vector1 <span style=color:#f92672>=</span> matrix1<span style=color:#f92672>.</span>dot(vector1)
</span></span><span style=display:flex><span>transformed_vector2 <span style=color:#f92672>=</span> matrix1<span style=color:#f92672>.</span>dot(vector2)
</span></span><span style=display:flex><span>plot_vectors([transformed_vector1, transformed_vector2], <span style=color:#e6db74>&#34;Transformed Vectors&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># we can see that the matrix has changed the initial vectors</span>
</span></span></code></pre></div><p><img loading=lazy src=./vectors_after_transformation_matplotlib_new.png alt="Vectors after linear transformation"></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plot_vectors([vector1, vector2, transformed_vector1, transformed_vector2], <span style=color:#e6db74>&#34;All vectors&#34;</span>)
</span></span></code></pre></div><p><img loading=lazy src=./all_vectors_superimposed_new.png alt="Vectors before and after transformation"></p><p>The rotation is more evident when we look at a plane. Borrowing an example from the Manga Guide to Linear Algebra book (follow the fish):</p><p>Initially, the vectors look like:<br><img loading=lazy src=./initial_vectors.png alt="Initial Vectors"></p><p>After the matrix multiplication, the vectors look like:<br><img loading=lazy src=./transformed_vectors.png alt="Vectors after transformation"></p><p>There&rsquo;s something interesting to notice in the graphs and the pictures. Notice that the plane rotates when we transform it linearly (multiply a matrix). Interestingly, the two vectors that we chose didn&rsquo;t rotate at all. This means all points on the original fish that were not on the lines shown in images were not just extended, but also rotated. However, the points on the two vectors that we chose were just extended. That&rsquo;s because they&rsquo;re special vectors with respect to this transformation (with respect to this matrix). These two vectors are the eigenvectors of the matrix, as multiplying them by the matrix given only changes their magnitude and not their direction. Which provides some intuition as to why we have:
$$\mathbf{A}x = \lambda x$$</p><p>When we have eigenvectors of the form shown in the graphs and pictures, we have eigenvalues associated with them. Eigenvectors and eigenvalues come in a pair. A matrix with dimensions $(m,m)$, can have at most $m$ eigenvectors and by extension $m$ eigenvalues.</p><p>We haven&rsquo;t yet found the eigenvalues for the example above. Let&rsquo;s try to generalize it.</p><h3 id=eigenvalues-generalized>Eigenvalues generalized<a hidden class=anchor aria-hidden=true href=#eigenvalues-generalized>#</a></h3><p>Starting with the equation we know:
$$\mathbf{A}x = \lambda x$$
Moving the terms to one side:
$$\mathbf{A} x-\lambda x=0$$
Factoring out the $x$, $I$ is the identity matrix. Realize that $\lambda$ is essentially a &ldquo;scalar&rdquo; since it just &ldquo;scales&rdquo; the vector $x$:
$$(\mathbf{A}-\lambda \mathbf{I}) x=0$$
For solutions where $x \neq 0$, we have:
$$\operatorname{det}(A-\lambda \mathbf{I})=0$$
We can only define determinants for square matrices.</p><p>$$
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1m} \\
a_{21} & a_{22} & \dots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mm}
\end{bmatrix}
$$
$$
\lambda \mathbf{I} =
\begin{bmatrix}
\lambda & 0 & \dots & 0 \\
0 & \lambda & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda
\end{bmatrix}
$$</p><p>$$
\mathbf{A} - \lambda \mathbf{I} =
\begin{bmatrix}
a_{11} - \lambda & a_{12} & \dots & a_{1m} \\
a_{21} & a_{22} - \lambda & \dots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mm} - \lambda
\end{bmatrix}
$$</p><p>$$
\det(\mathbf{A} - \lambda \mathbf{I}) = \begin{vmatrix}
a_{11} - \lambda & a_{12} & \dots & a_{1m} \\
a_{21} & a_{22} - \lambda & \dots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mm} - \lambda
\end{vmatrix}
$$</p><p>We know that $\det(\mathbf{A} - \lambda \mathbf{I}) = 0$, hence:
$$
\begin{vmatrix}
a_{11} - \lambda & a_{12} & \dots & a_{1m} \\
a_{21} & a_{22} - \lambda & \dots & a_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \dots & a_{mm} - \lambda
\end{vmatrix} = 0$$</p><p>Applying this general formula to our example:</p><p>$$\mathbf{A} = \begin{vmatrix}
8 & -3 \\
2 & 1 \
\end{vmatrix}$$</p><p>$$ \begin{vmatrix}
8 - \lambda & - 3 \\
2 & 1 - \lambda \
\end{vmatrix} = 0$$</p><p>$$ (8 - \lambda)(1 - \lambda) - (-3)(2) = 0$$</p><p>$$ 8 - 8\lambda - \lambda + \lambda^2 + 6 = 0$$</p><p>$$ \lambda^2 - 9\lambda + 14 = 0$$</p><p>Using the quadratic formula, we get:
$$ \lambda_1, \lambda_2 = \frac{9 \pm \sqrt{81 - 56}}{2}$$</p><p>$$ \lambda_1, \lambda_2 = \frac{9 \pm 5}{2}$$</p><p>$$ \lambda_1 = 7, \lambda_2 = 2$$</p><p>We have our eigenvalues. Neat. We already know the eigenvectors, since we chose them. However, if we didn&rsquo;t, we could find them using the equation $(\mathbf{A}-\lambda \mathbf{I})x=0$ or even $\mathbf{A}x =\lambda x$. Considering $\lambda_1 = 7$:</p><p>$$ \begin{bmatrix}
8 - 7 & - 3 \\
2 & 1 - 7 \
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$ \begin{bmatrix}
1 & - 3 \\
2 & -6 \
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$ \begin{bmatrix}
x_1 & - 3 x_2 \\
2 x_1 & -6 x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>We observe that the second row is just a multiple of the first row. This means that the second row is redundant and we can ignore it.</p><p>$$ \begin{bmatrix}
x_1 & - 3 x_2 \\
0 & 0 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$x_1 = 3 x_2$$</p><p>Hence,
$$x = \begin{bmatrix}
3 x_2 \\
x_2 \
\end{bmatrix}$$</p><p>Factorizing x_2, we get,
$$x = x_2 \begin{bmatrix}
3 \\
1 \
\end{bmatrix}$$</p><p>By varying x_2, we will obtain the different scaled versions of the eigenvector. Which leaves with with the eigenvector: [3, 1] expressed in x-y cartesian co-ordinates as x = 3, y = 1. Which is the same as vector_1 😄! Yay!</p><p>We do the same thing with $\lambda_2=2$:
$$ \begin{bmatrix}
8 - 2 & - 3 \\
2 & 1 - 2 \
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$ \begin{bmatrix}
6 & - 3 \\
2 & -1 \
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$ \begin{bmatrix}
6 x_1 & - 3 x_2 \\
2 x_1 & -1 x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>The top row is a multiple of the bottom row, ignoring:
$$ \begin{bmatrix}
0 & 0 \\
2 x_1 & -1 x_2 \
\end{bmatrix}
= \begin{bmatrix}
0 \\
0 \
\end{bmatrix}$$</p><p>$$2 x_1 = x_2$$</p><p>$$x = \begin{bmatrix}
x_1 \\
2 x_1 \
\end{bmatrix}$$</p><p>Factorizing x_1, we get,
$$x = x_1 \begin{bmatrix}
1 \\
2 \
\end{bmatrix}$$</p><p>And we have our second eigenvector: [1, 2] expressed in x-y cartesian co-ordinates as x = 1, y = 2. Which is the same as vector_2!</p><h2 id=powers-of-a-matrix>Powers of a matrix<a hidden class=anchor aria-hidden=true href=#powers-of-a-matrix>#</a></h2><p>Say we had to calculate $2^{12}$ and we weren&rsquo;t allowed to use a calculator. We could multiply 2 by hand 12 times, or we could try to be smart and use the fact that $2^{12} = 2^4 * 2^4 * 2^4$. But what if I had to find the 12th power of a matrix? Even multiplying a matrix a couple of times by hand is a pain. We need a trick, much like the one we used for $2^{12}$.</p><p>What if we had a m-dimensional square diagonal matrix (mxm) like the one below:
$$\mathbf{D} =
\begin{bmatrix}
d_1 & 0 & \dots & 0 \\
0 & d_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_m
\end{bmatrix}$$</p><p>It would be quite easy to raise this to the second power:
$$\mathbf{D}^2 = \begin{bmatrix}
d_1^2 & 0 & \dots & 0 \\
0 & d_2^2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_m^2
\end{bmatrix}$$</p><p>Or even the 100th power:
$$\mathbf{D}^{100} = \begin{bmatrix}
d_1^{100} & 0 & \dots & 0 \\
0 & d_2^{100} & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & d_m^{100}
\end{bmatrix}$$</p><h3 id=diagonal-matrices-and-eigenvalues>Diagonal matrices and eigenvalues<a hidden class=anchor aria-hidden=true href=#diagonal-matrices-and-eigenvalues>#</a></h3><p>Diagonal matrices are nice, since they only change the magnitude of a vector. For instance:
$$\begin{bmatrix}
d_1 & 0 & 0 \\
0 & d_2 & 0 \\
0 & 0 & d_3
\end{bmatrix}
\begin{bmatrix}
a_1 \\
0 \\
0 \
\end{bmatrix}
= d_1\begin{bmatrix}
a_1 \\
0 \\
0 \
\end{bmatrix}
$$</p><p>$$\begin{bmatrix}
d_1 & 0 & 0 \\
0 & d_2 & 0 \\
0 & 0 & d_3
\end{bmatrix}
\begin{bmatrix}
0 \\
b_1 \\
0 \
\end{bmatrix}
= d_2\begin{bmatrix}
0 \\
b_1 \\
0 \
\end{bmatrix}
$$</p><p>$$\begin{bmatrix}
d_1 & 0 & 0 \\
0 & d_2 & 0 \\
0 & 0 & d_3
\end{bmatrix}
\begin{bmatrix}
0 \\
0 \\
c_1 \
\end{bmatrix}
= d_3\begin{bmatrix}
0 \\
0 \\
c_1 \
\end{bmatrix}
$$</p><p>What if we had a diagonal matrix that was only composed of eigenvalues? That would be quite nice and it seems like concept comes about naturally since diagonal matrix also scale vectors, just like eigenvalue of a matrix scale the eigenvectors.</p><p>From $\mathbf{A}x = \lambda x$, we know that for an eigenvector $x$, multiplication by a matrix is the same as being multiplied by a scalar $\lambda$. Multiplying a matrix and its eigenvector several times would be the same as multiplying the scalar $\lambda$ several times.</p><p>We still need to figure out how to find higher powers for non-diagonal matrices. Perhaps we could factorize this non-diagonal matrix into a product of other matrices including a diagonal matrix? Maybe we could use the eigenvectors too since their scaling laws are known ? Turns out there&rsquo;s a process to do just that, called diagonalization. Starting out with the familiar eigenvalue equation:
$$\mathbf{A} \mathbf{X}= \lambda x \tag{Eigenvalue equation}$$</p><p>The magic equation turns out to &ldquo;look&rdquo; very similar:
$$\mathbf{A} \mathbf{X}= \mathbf{X} \mathbf{\Lambda} \tag{Diagonalization equation}$$</p><p>$\mathbf{A}$ is the original square matrix that we&rsquo;ve been dealing with. But unlike the eigenvalue equation where $x$ was a vector and $\lambda$ was a scalar, $\mathbf{X}$ and $\Lambda$ are both matrices. $\mathbf{X}$ is matrix containing the eigenvectors of $\mathbf{A}$ as its columns. This equation will help us factorize $\mathbf{A}$ into a product of other matrices. Please note that in this case $\mathbf{X}$ is not the input matrix, just a generic matrix representing the eigenvectors of $\mathbf{A}$.</p><p>Mathematical magic, starting with the diagonalization equation:
$$\mathbf{A} \mathbf{X}= \mathbf{X} \mathbf{\Lambda}$$
$$\mathbf{X}^{-1} \mathbf{A} \mathbf{X} = \mathbf{X}^{-1} \mathbf{X} \mathbf{\Lambda}$$
A matrix and its inverse colliding is like matter and anti-matter coming into contact, they both annihilate each other, albeit without the explosion. Hence:
$$\mathbf{X}^{-1} \mathbf{A} \mathbf{X} = \Lambda$$
$\Lambda$ is a diagonal matrix with the eigenvalues of $\mathbf{A}$ on the diagonal! Hence the matrix A is diagonalized!</p><p>When we want to find the powers of $\mathbf{A}$, we can use the diagonalization equation, going the other way around:
$$\mathbf{A} \mathbf{X}= \mathbf{X} \mathbf{\Lambda}$$
$$\mathbf{A} \mathbf{X} \mathbf{X^{-1}}= \mathbf{X} \mathbf{\Lambda} \mathbf{X^{-1}}$$
$$\mathbf{A}= \mathbf{X} \mathbf{\Lambda} \mathbf{X^{-1}}$$</p><p>Since it doesn&rsquo;t matter to the eigenvectors what they&rsquo;re being scaled by and since $\Lambda$ is a diagonal matrix:
$$\mathbf{A}^k= \mathbf{X} \mathbf{\Lambda}^k \mathbf{X^{-1}}$$</p><p>Et voilà! We can finally do PCA!</p><h2 id=finally-pca>Finally, PCA<a hidden class=anchor aria-hidden=true href=#finally-pca>#</a></h2><p>Along the course of this journey, we only encountered one matrix, the covariance matrix. Recall that the covariance matrix will always be a square matrix, i.e. it will have the dimensions $(m,m)$. We can diagonalize the covariance matrix (remember that the covariance matrix is denoted by $\Sigma$):
$$\mathbf{\Sigma}= \mathbf{P} \mathbf{\Lambda} \mathbf{P^{-1}}$$
where $\Lambda$ is a diagonal matrix with the eigenvalues of $\mathbf{\Sigma}$ and $\mathbf{P}$ is a matrix with the eigenvectors of $\mathbf{\Sigma}$ as its columns.</p><p>You could also call the diagonalization of a matrix its eigenvalue decomposition, because when we diagonalize a matrix, we express it in terms of its eigenvalues and eigenvectors.</p><p>Our premise is that we will convert $m$ dimensional data to $n$ dimensional data. Very simply, if I had a $k$ such $m$ dimensional points $(k,m)$ and I wished to project / transform to $k$ points to $n$ dimensions, I could very naively multiply the original $(k,m)$ matrix by a $(m,n)$ dimensional matrix to obtain a $(k,n)$ dimensional matrix.
$$(k,m)\cdot(m,n) \rightarrow (k,n)$$</p><p>To obtain the $(m,n)$ dimensional matrix, we just need to use the covariance matrix which is $(m,m)$ dimensional.</p><p>$$\mathbf{\Sigma} = \mathbf{P} \mathbf{\Lambda} \mathbf{P^{-1}}$$</p><p>Sort the eigenvalues in descending order.
$$\mathbf{\Lambda_{\text{sorted}}} = \text{sort}(\mathbf{\Lambda}, \text{descending})$$</p><p>We use these eigenvalues to then pick the top $n$ eigenvectors corresponding to the top eigenvalues.
$$\mathbf{\Lambda_n} = \text{diag}(\mathbf{\Lambda_{\text{sorted}}[:n]})$$
$$\mathbf{\Lambda_n} \rightarrow \mathbf{P_n}$$
$$\mathbf{P_n} = \mathbf{P[:, :n]}$$
The notation at the end is just numpy notation for picking all rows and the first $n$ columns of the matrix $\mathbf{P}$.</p><p>$P_n$ is the projection matrix. We then multiply the original $(k,m)$ dimensional matrix by the $(m,n)$ dimensional projection matrix to obtain the $(k,n)$ dimensional matrix. Is this the end of the story? Are we finally done with this long long post? Do I just simply multiply the input matrix X?
$$\mathbf{X} \mathbf{P_n} \rightarrow (k,n)$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># let&#39;s look at some code to answer this question:</span>
</span></span><span style=display:flex><span>n_samples <span style=color:#f92672>=</span> <span style=color:#ae81ff>15</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(n_samples,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>11</span>):
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>hstack((X, (<span style=color:#ae81ff>10</span><span style=color:#f92672>**</span>_)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(n_samples,<span style=color:#ae81ff>1</span>)))
</span></span><span style=display:flex><span>X<span style=color:#f92672>.</span>shape
</span></span></code></pre></div><pre><code>(15, 10)
</code></pre><p>In the previous cell, I created some inputs X which has n_samples number of samples and each column&rsquo;s magnitude is 10 times the previous column&rsquo;s magnitude.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pca</span>(X: np<span style=color:#f92672>.</span>array, k: int, standardize:bool<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>array:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Finds pca. Can select whether or not to standarize the data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        X (np.array): Input data.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        k (int): Number of principal components to retain.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        standarize (bool, optional): Flag to choose whether to standardize. Defaults to True.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        np.array: projected data
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># let&#39;s find the mean for each feature</span>
</span></span><span style=display:flex><span>    mean <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>mean(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># subtract the mean from each feature;  shape is broadcasted</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> standardize:
</span></span><span style=display:flex><span>        X <span style=color:#f92672>=</span> X <span style=color:#f92672>-</span> mean
</span></span><span style=display:flex><span>    covariance_matrix <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X<span style=color:#f92672>.</span>T, X) <span style=color:#f92672>/</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># find eigenvalues and eigenvectors; </span>
</span></span><span style=display:flex><span>    eigenvalues, eigenvectors <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>eig(covariance_matrix)
</span></span><span style=display:flex><span>    <span style=color:#75715e># sort the eigenvectors by decreasing eigenvalues</span>
</span></span><span style=display:flex><span>    eigenvectors <span style=color:#f92672>=</span> eigenvectors[:, np<span style=color:#f92672>.</span>argsort(eigenvalues)[::<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]]
</span></span><span style=display:flex><span>    <span style=color:#75715e># multiply by the input</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> standardize:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>dot(X, eigenvectors[:, :k])<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sqrt(eigenvalues[:k])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>dot(X, eigenvectors[:, :k])
</span></span></code></pre></div><p>In the cell above, we define a PCA function. To compute the covariance matrix, instead of using np.cov(X, rowvar=False), we use the formula for covariance just for practice. The dot product method is kinda neat, but it run in numerical instabilities (read as divide by zero errors). So overall, you might wanna stick to np.cov(X, rowvar=False). But anyway to explain the dot product - X is $(m,n)$ and we need a $(n,n)$ covariance matrix. We need to multiply every value in X by every other value in X and sum the results. This is the same as multiplying X by its transpose. The transpose of X, X.T is $(n,m)$. The dot product of transpose of X and X is $(n,m)$ x $(m,n)$ = $(n,n)$ . We divide by X.shape[0] to get the mean. Let&rsquo;s try compute PCA with and without standardization:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pca(X, <span style=color:#ae81ff>3</span>,standardize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><pre><code>array([[-7.87659032e+09,  1.21888717e+08,  9.32299737e+06],
       [-3.81517032e+09, -7.26896071e+08,  1.23143528e+07],
       [-4.20032164e+09, -5.04772750e+08,  4.93815085e+06],
       [-3.27768073e+09, -7.81313407e+07, -8.53722860e+06],
       [-5.24910853e+09, -4.10123167e+08, -3.68877997e+07],
       [-9.61861820e+09,  2.10930492e+08,  1.95338785e+07],
       [-2.44899771e+09, -2.57069015e+08, -3.33657203e+07],
       [-8.72847390e+09,  2.47534780e+08,  1.22615401e+07],
       [-4.21212718e+09,  2.84941041e+08, -5.27834809e+07],
       [-3.85027490e+09,  2.87810264e+07, -5.64613414e+07],
       [-8.99977542e+09,  6.58608889e+07,  3.38969959e+07],
       [-9.33259978e+09, -6.94629542e+06,  1.36187000e+07],
       [-1.07794112e+09, -2.85311558e+08, -4.32094387e+07],
       [-5.15673575e+09, -2.24746557e+08,  3.51379560e+07],
       [-7.50218395e+09,  3.21229490e+08, -3.46309614e+07]])
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pca(X, <span style=color:#ae81ff>3</span>, standardize<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><pre><code>array([[-0.82283229, -0.23898001,  0.34658066],
       [ 0.71682725,  2.11362558,  0.04130555],
       [ 0.56736939,  1.31895532,  0.15915883],
       [ 0.90200762, -0.64220242,  1.31329582],
       [ 0.17293128,  1.20216573, -2.24997976],
       [-1.47652018, -0.13882577,  0.21043122],
       [ 1.21636054, -0.14541439, -0.06016706],
       [-1.14450273, -0.52078503,  0.42350452],
       [ 0.54376545, -1.86936294, -0.31600902],
       [ 0.68537049, -0.93441812, -1.12269834],
       [-1.24157013,  0.28645995,  0.81769758],
       [-1.36420737,  0.66142018, -0.64117307],
       [ 1.72982373, -0.39302796,  0.09484229],
       [ 0.20275971,  0.45025174,  2.05781732],
       [-0.68758276, -1.14986187, -1.07460652]])
</code></pre><p>Observe that we get way more amenable projected values when we choose to standardize despite having features in several different scales. PCA is sensitive to the scale of the features entered, so standardizing ensures that we do not assign higher importance to larger features. Standardizing also helps you avoid overflow errors, i.e. numbers that are too large to be represented by your selected datatype. Typically, to standardize you subtract the mean and divide by the standard deviation.
$$X_{standardized} = \frac{{X - \mu}}{{\sigma}}$$</p><p>So, finally, we express PCA as:
$$
\mathbf{Z}=\mathbf{\Lambda}_n^{-\frac{1}{2}} (\mathbf{X}-\overline{\mathbf{X}})\mathbf{P}_n
$$
$$
\mathbf{Z}=\mathbf{\Lambda}_n^{-\frac{1}{2}} \mathbf{P}_n^T(\mathbf{X}-\overline{\mathbf{X}})
$$
$\mathbf{Z}$ is the projection of $\mathbf{X}$ in the lower dimensional space. $\mathbf{\Lambda}_n^{-\frac{1}{2}}$ is the inverse square root of the eigenvalues. This is where we&rsquo;re dividing everything by the standard deviation. $\mathbf{P}_n$ is the projection matrix. $\mathbf{X}$ is the original data. $\overline{\mathbf{X}}$ is the mean of the original data. The two forms are equivalent, we just need to change the order based on the shape of matrix. Standard deviation is the root of the variance, so the division by the square root of the eigenvalue matrix (diagonal matrix) or multiplication by the negative half power, is the same as dividing by the standard deviation. In essence, we&rsquo;re just standardizing the data for the reasons we demonstrated in code.</p><h2 id=what-next>What next?<a hidden class=anchor aria-hidden=true href=#what-next>#</a></h2><ul><li>Well we just linearly transformed our data into lower dimensions. You could learn about Kernel PCA for non-linear transformations.</li><li>Just a note, matrices with repeated eigenvalues cannot be diagonalized. So not all matrices can be diagonalized with this approach. I believe that most PCA implementations nowadays use SVD instead of eigenvalue decompositions since SVD can work even with matrices that have repeated eigenvalues. You could learn about SVD.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Slides by Pascal Vincet and explanations by Ioannis Mitliagkas in the course IFT630 - Fundamentals of Machine Learning taught at the University of Montreal.</li><li><a href=(https://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf)>A tutorial on Principal Component Analysis by Lindsay I Smith at the University of Montreal</a>.</li><li><a href=https://psycnet.apa.org/record/1934-00645-001>Analysis of a complex of statistical variables into principal components by Harold Hotelling.</a></li><li><a href="https://www.researchgate.net/publication/316652806_Principal_Component_Analysis#:~:text=Principal%20component%20analysis%20(PCA)%20is,inter%2Dcorrelated%20quantitative%20dependent%20variables.">PCA - International Journal of Livestock Research.</a></li><li><a href=https://sebastianraschka.com/Articles/2014_pca_step_by_step.html>Implementing a Principal Component Analysis (PCA) - Sebastian Raschka.</a></li><li>Introduction to Linear Algebra by Gilbert Strang.</li><li>Interactive Linear Algebra by Dan Margalit and Joseph Rabinoff.</li><li>The Mange Guide to Linear Algebra by Shin Takahashi and Iroha Inoue.</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://etrama.github.io/tags/dimensionality-reduction/>Dimensionality Reduction</a></li><li><a href=https://etrama.github.io/tags/pca/>PCA</a></li><li><a href=https://etrama.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://etrama.github.io/tags/linear-algebra/>Linear Algebra</a></li></ul><nav class=paginav><a class=next href=https://etrama.github.io/posts/2021-12-23-nhl-ds-project/><span class=title>Next »</span><br><span>NHL Data Science Project</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on twitter" href="https://twitter.com/intent/tweet/?text=Demystifying%20the%20mathematics%20behind%20PCA&amp;url=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f&amp;hashtags=DimensionalityReduction%2cPCA%2cMachineLearning%2cLinearAlgebra"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f&amp;title=Demystifying%20the%20mathematics%20behind%20PCA&amp;summary=Demystifying%20the%20mathematics%20behind%20PCA&amp;source=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f&title=Demystifying%20the%20mathematics%20behind%20PCA"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on whatsapp" href="https://api.whatsapp.com/send?text=Demystifying%20the%20mathematics%20behind%20PCA%20-%20https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on telegram" href="https://telegram.me/share/url?text=Demystifying%20the%20mathematics%20behind%20PCA&amp;url=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Demystifying the mathematics behind PCA on ycombinator" href="https://news.ycombinator.com/submitlink?t=Demystifying%20the%20mathematics%20behind%20PCA&u=https%3a%2f%2fetrama.github.io%2fposts%2f2023-10-07-pca%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://etrama.github.io/>Old Habits AI Hard</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>